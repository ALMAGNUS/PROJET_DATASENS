# üèóÔ∏è Architecture Buffer SQLite - Documentation Technique

## üìã Vue d'Ensemble

Ce document explique l'architecture technique du **buffer SQLite** (`datasens.db`) et son cycle de vie dans le syst√®me DataSens E1/E2/E3.

**Niveau** : Senior Developer / Architect  
**Principes** : OOP, SOLID, DRY  
**S√©curit√©** : Garanties de non-perte de donn√©es

---

## üéØ Concept Architectural

### Buffer SQLite = Zone de Transit Temporaire

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ARCHITECTURE BUFFER                       ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  SOURCES EXTERNES (14 sources)                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - RSS, APIs, Web Scraping, Datasets               ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îÇ                          ‚îÇ COLLECTE                          ‚îÇ
‚îÇ                          ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  BUFFER SQLite (datasens.db) - TEMPORAIRE          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚ö†Ô∏è Zone de transit - NE PAS CONSERVER               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Tables:                                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - raw_data (articles bruts)                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - document_topic (enrichissement topics)          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - model_output (enrichissement sentiment)         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - source, topic, sync_log (m√©tadonn√©es)          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  R√¥le:                                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  1. Collecte quotidienne                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  2. Enrichissement (topics + sentiment)            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  3. Export vers Parquet GOLD                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  4. Nettoyage (apr√®s export v√©rifi√©)                ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îÇ                          ‚îÇ EXPORT (GoldExporter)             ‚îÇ
‚îÇ                          ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  STOCKAGE PERMANENT Parquet GOLD                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ Source de v√©rit√©                                ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ Immutable (une fois cr√©√©, ne change pas)        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ Partitionn√© par date                            ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                          ‚îÇ                                   ‚îÇ
‚îÇ                          ‚îÇ LECTURE (PySpark)                 ‚îÇ
‚îÇ                          ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  PYSPARK (E2/E3)                                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - GoldParquetReader (lecture seule)               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - GoldDataProcessor (analyses)                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - API Endpoints (analytics)                        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîí Garanties de S√©curit√©

### Principe : Export Avant Nettoyage

**R√®gle d'or** : Les donn√©es ne sont **JAMAIS** supprim√©es du buffer SQLite avant d'√™tre **v√©rifi√©es** dans Parquet GOLD.

### M√©canismes de Protection

#### 1. V√©rification Export Parquet (Ligne 45-60)

```python
def check_parquet_export(target_date: date | None = None) -> bool:
    """V√©rifie qu'un export Parquet existe pour la date donn√©e"""
    check_date = target_date or date.today()
    parquet_path = Path('data/gold') / f"date={check_date:%Y-%m-%d}" / "articles.parquet"
    
    if parquet_path.exists():
        import pyarrow.parquet as pq
        try:
            num_rows = pq.ParquetFile(parquet_path).metadata.num_rows
            print(f"  ‚úÖ Parquet trouve: {parquet_path}")
            print(f"     {num_rows:,} lignes")
            return True
        except Exception as e:
            print(f"  ‚ö†Ô∏è Parquet existe mais erreur lecture: {e}")
            return False
    else:
        print(f"  ‚ùå Parquet non trouve: {parquet_path}")
        return False
```

**Garanties** :
- ‚úÖ V√©rifie l'existence du fichier Parquet
- ‚úÖ V√©rifie que le fichier est lisible (pas corrompu)
- ‚úÖ Affiche le nombre de lignes pour validation
- ‚ùå **Bloque le nettoyage si Parquet absent ou corrompu**

#### 2. Confirmation Utilisateur (Ligne 200-210)

```python
# V√©rifier export Parquet
print("\nVerification export Parquet GOLD...")
if not check_parquet_export():
    print("\n‚ö†Ô∏è ATTENTION: Aucun export Parquet trouve pour aujourd'hui!")
    confirm = input("   Continuer quand meme? (o/n): ").strip().lower()
    if confirm != 'o':
        print("   Nettoyage annule")
        sys.exit(0)
```

**Garanties** :
- ‚úÖ Demande confirmation explicite si Parquet manquant
- ‚úÖ Permet d'annuler le nettoyage
- ‚úÖ Avertissement clair avant action destructive

#### 3. Mode Simulation (Dry-Run) (Ligne 220-250)

```python
elif choice == "3":
    keep_days_input = input("Nombre de jours a garder (defaut: 7): ").strip()
    keep_days = int(keep_days_input) if keep_days_input else 7
    
    print(f"\nSIMULATION: Nettoyage des articles plus anciens que {keep_days} jours...")
    result = cleanup_buffer(db_path, keep_days=keep_days, dry_run=True)
    print("\n‚ö†Ô∏è SIMULATION - Aucune donnee n'a ete supprimee")
```

**Garanties** :
- ‚úÖ Permet de voir ce qui sera supprim√© **sans supprimer**
- ‚úÖ Affiche les statistiques avant/apr√®s (simulation)
- ‚úÖ Aucune modification de la base de donn√©es en mode simulation

---

## üèóÔ∏è Architecture du Code

### Principe SOLID Appliqu√©

#### 1. Single Responsibility Principle (SRP)

Chaque fonction a une responsabilit√© unique :

```python
def get_db_path() -> str | None:
    """Trouve le chemin de la base de donn√©es"""
    # Responsabilit√© UNIQUE: Trouver le chemin DB
    # Ne fait QUE √ßa, rien d'autre

def check_parquet_export(target_date: date | None = None) -> bool:
    """V√©rifie qu'un export Parquet existe"""
    # Responsabilit√© UNIQUE: V√©rifier export Parquet
    # Ne fait QUE √ßa, rien d'autre

def get_db_stats(conn: sqlite3.Connection) -> dict:
    """R√©cup√®re les statistiques de la base de donn√©es"""
    # Responsabilit√© UNIQUE: R√©cup√©rer stats
    # Ne fait QUE √ßa, rien d'autre

def cleanup_buffer(...) -> dict:
    """Nettoie le buffer SQLite"""
    # Responsabilit√© UNIQUE: Nettoyer le buffer
    # Ne fait QUE √ßa, rien d'autre
```

#### 2. Open/Closed Principle (OCP)

Le code est extensible sans modification :

```python
def cleanup_buffer(
    db_path: str,
    keep_days: int = 7,
    target_date: date | None = None,  # ‚Üê Extension possible
    dry_run: bool = False              # ‚Üê Extension possible
) -> dict:
    """Nettoie le buffer SQLite"""
    # Peut √™tre √©tendu avec de nouveaux param√®tres
    # Sans modifier le code existant
```

#### 3. Dependency Inversion Principle (DIP)

Les d√©pendances sont inject√©es, pas hardcod√©es :

```python
def cleanup_buffer(db_path: str, ...):  # ‚Üê Injection de d√©pendance
    conn = sqlite3.connect(db_path)     # ‚Üê Utilise l'injection
    # Pas de chemin hardcod√© dans la fonction
```

### Principe DRY (Don't Repeat Yourself)

#### √âviter la Duplication

```python
# ‚ùå MAUVAIS (duplication)
if target_date:
    cursor.execute("DELETE FROM raw_data WHERE date(collected_at) = date(?)", (date_str,))
    deleted_raw = cursor.rowcount
    cursor.execute("DELETE FROM document_topic WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
    deleted_topics = cursor.rowcount
    cursor.execute("DELETE FROM model_output WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
    deleted_sentiment = cursor.rowcount
else:
    cursor.execute("DELETE FROM raw_data WHERE collected_at < date('now', '-' || ? || ' days')", (keep_days,))
    deleted_raw = cursor.rowcount
    cursor.execute("DELETE FROM document_topic WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
    deleted_topics = cursor.rowcount
    cursor.execute("DELETE FROM model_output WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
    deleted_sentiment = cursor.rowcount

# ‚úÖ BON (DRY - pas de duplication)
if target_date:
    # Supprimer articles de cette date
    cursor.execute("DELETE FROM raw_data WHERE date(collected_at) = date(?)", (date_str,))
    deleted_raw = cursor.rowcount
else:
    # Supprimer articles anciens
    cursor.execute("DELETE FROM raw_data WHERE collected_at < date('now', '-' || ? || ' days')", (keep_days,))
    deleted_raw = cursor.rowcount

# Nettoyage tables li√©es (COMMUN aux deux cas)
cursor.execute("DELETE FROM document_topic WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
deleted_topics = cursor.rowcount

cursor.execute("DELETE FROM model_output WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
deleted_sentiment = cursor.rowcount
```

---

## üîÑ Workflow D√©taill√©

### √âtape 1 : Collecte Quotidienne (E1 Pipeline)

```python
# src/e1/pipeline.py
def run(self):
    # 1. Extraction depuis sources
    articles = self.extract()  # ‚Üí raw_data (SQLite)
    
    # 2. Nettoyage
    articles = self.clean(articles)  # ‚Üí raw_data (SQLite)
    
    # 3. Chargement dans buffer SQLite
    self.load(articles)  # ‚Üí raw_data (SQLite)
    
    # 4. Enrichissement topics
    # ‚Üí document_topic (SQLite)
    
    # 5. Enrichissement sentiment
    # ‚Üí model_output (SQLite)
```

**R√©sultat** : Buffer SQLite contient les donn√©es enrichies

### √âtape 2 : Export Buffer ‚Üí Parquet GOLD

```python
# src/e1/pipeline.py
aggregator = DataAggregator(db_path)
exporter = GoldExporter()

# Agr√©gation GOLD (JOINs SQL)
df_gold = aggregator.aggregate()
# ‚Üí Joint raw_data + document_topic + model_output

# Export Parquet
result = exporter.export_all(df_gold, date.today())
# ‚Üí data/gold/date=2025-12-20/articles.parquet
```

**R√©sultat** : Donn√©es sauvegard√©es dans Parquet GOLD (stockage permanent)

### √âtape 3 : V√©rification Export (S√©curit√©)

```python
# scripts/cleanup_sqlite_buffer.py
if not check_parquet_export():
    # ‚ö†Ô∏è Parquet manquant ‚Üí Bloque nettoyage
    confirm = input("Continuer quand meme? (o/n): ")
    if confirm != 'o':
        sys.exit(0)  # ‚Üê Annule nettoyage
```

**Garantie** : Pas de nettoyage si Parquet manquant

### √âtape 4 : Nettoyage Buffer (Optionnel)

```python
# scripts/cleanup_sqlite_buffer.py
cleanup_buffer(db_path, keep_days=7, dry_run=False)
```

**Ce qui se passe** :
1. Supprime `raw_data` (articles anciens)
2. Supprime `document_topic` (orphans apr√®s suppression raw_data)
3. Supprime `model_output` (orphans apr√®s suppression raw_data)
4. **Garde** `source`, `topic`, `sync_log` (m√©tadonn√©es)

---

## üõ°Ô∏è Int√©grit√© R√©f√©rentielle

### Tables Li√©es et Cascades

```
raw_data (table principale)
    ‚îÇ
    ‚îú‚îÄ‚îÄ document_topic (FK: raw_data_id)
    ‚îÇ       ‚îî‚îÄ‚îÄ topic (FK: topic_id)
    ‚îÇ
    ‚îî‚îÄ‚îÄ model_output (FK: raw_data_id)
```

**Strat√©gie de Nettoyage** :

1. **Supprimer d'abord `raw_data`** (table principale)
   ```sql
   DELETE FROM raw_data WHERE collected_at < date('now', '-7 days')
   ```

2. **Nettoyer les orphans** (tables li√©es)
   ```sql
   DELETE FROM document_topic 
   WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)
   
   DELETE FROM model_output 
   WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)
   ```

**Pourquoi cet ordre ?**
- ‚úÖ Respecte l'int√©grit√© r√©f√©rentielle
- ‚úÖ √âvite les erreurs de contraintes FK
- ‚úÖ Nettoie proprement les donn√©es orphelines

---

## üìä Statistiques et Monitoring

### Avant Nettoyage

```python
stats_before = get_db_stats(conn)
# {
#     'raw_data_count': 43022,
#     'document_topic_count': 86044,
#     'model_output_count': 43022,
#     'oldest_date': '2025-12-16',
#     'newest_date': '2025-12-20'
# }
```

### Apr√®s Nettoyage

```python
stats_after = get_db_stats(conn)
# {
#     'raw_data_count': 5000,  # ‚Üê R√©duit (articles > 7 jours supprim√©s)
#     'document_topic_count': 10000,  # ‚Üê R√©duit (orphans nettoy√©s)
#     'model_output_count': 5000,  # ‚Üê R√©duit (orphans nettoy√©s)
#     'oldest_date': '2025-12-13',  # ‚Üê Mis √† jour
#     'newest_date': '2025-12-20'
# }
```

### Calcul des Suppressions

```python
deleted_raw = stats_before['raw_data_count'] - stats_after['raw_data_count']
deleted_topics = stats_before['document_topic_count'] - stats_after['document_topic_count']
deleted_sentiment = stats_before['model_output_count'] - stats_after['model_output_count']
```

---

## üîç Analyse du Code Ligne par Ligne

### Fonction `cleanup_buffer()` - Lignes 100-200

```python
def cleanup_buffer(
    db_path: str,                    # ‚Üê Injection d√©pendance (DIP)
    keep_days: int = 7,              # ‚Üê Param√®tre configurable
    target_date: date | None = None, # ‚Üê Flexibilit√© (OCP)
    dry_run: bool = False            # ‚Üê S√©curit√© (simulation)
) -> dict:
    """
    Nettoie le buffer SQLite
    
    Args:
        db_path: Chemin vers la base de donn√©es
        keep_days: Nombre de jours √† garder (articles plus r√©cents)
        target_date: Date sp√©cifique √† nettoyer (optionnel)
        dry_run: Si True, simule sans supprimer
    
    Returns:
        Dictionnaire avec statistiques de nettoyage
    """
    # 1. Connexion DB (injection d√©pendance)
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # 2. Statistiques AVANT (baseline)
    stats_before = get_db_stats(conn)
    
    # 3. Logique de nettoyage (SRP: une seule responsabilit√©)
    if target_date:
        # Nettoyage par date sp√©cifique
        cursor.execute("""
            DELETE FROM raw_data 
            WHERE date(collected_at) = date(?)
        """, (target_date.isoformat(),))
        deleted_raw = cursor.rowcount
    else:
        # Nettoyage par anciennet√©
        cursor.execute("""
            DELETE FROM raw_data 
            WHERE collected_at < date('now', '-' || ? || ' days')
        """, (keep_days,))
        deleted_raw = cursor.rowcount
    
    # 4. Nettoyage tables li√©es (DRY: code commun)
    cursor.execute("""
        DELETE FROM document_topic 
        WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)
    """)
    deleted_topics = cursor.rowcount
    
    cursor.execute("""
        DELETE FROM model_output 
        WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)
    """)
    deleted_sentiment = cursor.rowcount
    
    # 5. Commit (si pas dry_run)
    if not dry_run:
        conn.commit()
    
    # 6. Statistiques APR√àS
    stats_after = get_db_stats(conn)
    
    # 7. Retour r√©sultats
    return {
        'deleted_raw': deleted_raw,
        'deleted_topics': deleted_topics,
        'deleted_sentiment': deleted_sentiment,
        'stats_before': stats_before,
        'stats_after': stats_after
    }
```

**Points Cl√©s** :
- ‚úÖ **SRP** : Une seule responsabilit√© (nettoyer le buffer)
- ‚úÖ **DIP** : Injection de `db_path` (pas de hardcode)
- ‚úÖ **OCP** : Extensible (`target_date`, `dry_run`)
- ‚úÖ **DRY** : Pas de duplication (nettoyage tables li√©es commun)
- ‚úÖ **S√©curit√©** : `dry_run` pour simulation

---

## ‚ö†Ô∏è Points d'Attention

### 1. Transaction SQLite

**Probl√®me potentiel** : Si erreur pendant nettoyage, donn√©es partiellement supprim√©es

**Solution actuelle** : Utilise `conn.commit()` √† la fin (transaction implicite)

**Am√©lioration possible** :
```python
try:
    # Nettoyage...
    conn.commit()  # ‚Üê Commit seulement si tout OK
except Exception as e:
    conn.rollback()  # ‚Üê Rollback en cas d'erreur
    raise
```

### 2. V√©rification Parquet

**Probl√®me potentiel** : Parquet peut exister mais √™tre corrompu

**Solution actuelle** : V√©rifie existence + lisibilit√© (ligne 50-55)

**Am√©lioration possible** :
```python
def check_parquet_export(target_date: date | None = None) -> bool:
    # V√©rifier existence
    if not parquet_path.exists():
        return False
    
    # V√©rifier lisibilit√©
    try:
        pq.ParquetFile(parquet_path).metadata.num_rows
    except Exception:
        return False
    
    # V√©rifier nombre de lignes > 0
    if num_rows == 0:
        return False
    
    return True
```

### 3. Backup Avant Nettoyage

**Recommandation** : Cr√©er un backup SQLite avant nettoyage

```python
def create_backup(db_path: str) -> str:
    """Cr√©e un backup de la base de donn√©es"""
    backup_path = f"{db_path}.backup.{date.today().isoformat()}"
    import shutil
    shutil.copy2(db_path, backup_path)
    return backup_path
```

---

## üìù Checklist Avant Nettoyage

### ‚úÖ V√©rifications Obligatoires

1. **Export Parquet existe** ‚úÖ
   ```python
   check_parquet_export()  # ‚Üí True
   ```

2. **Parquet lisible** ‚úÖ
   ```python
   pq.ParquetFile(parquet_path).metadata.num_rows  # ‚Üí > 0
   ```

3. **Confirmation utilisateur** ‚úÖ
   ```python
   confirm = input("Continuer? (o/n): ")  # ‚Üí 'o'
   ```

4. **Mode simulation test√©** ‚úÖ
   ```python
   cleanup_buffer(..., dry_run=True)  # ‚Üí Test√© d'abord
   ```

### ‚ö†Ô∏è V√©rifications Recommand√©es

5. **Backup SQLite cr√©√©** (optionnel mais recommand√©)
6. **V√©rifier nombre de lignes Parquet vs SQLite** (coh√©rence)
7. **V√©rifier dates dans Parquet** (coh√©rence)

---

## üîß Utilisation Recommand√©e

### Workflow S√©curis√©

```bash
# 1. Ex√©cuter pipeline E1 (collecte + export)
python main.py
# ‚Üí Collecte dans buffer SQLite
# ‚Üí Export vers Parquet GOLD

# 2. V√©rifier export Parquet
python -c "from pathlib import Path; import pyarrow.parquet as pq; \
    p = Path('data/gold/date=2025-12-20/articles.parquet'); \
    print(f'Parquet: {pq.ParquetFile(p).metadata.num_rows} lignes')"

# 3. Simulation nettoyage (dry-run)
python scripts/cleanup_sqlite_buffer.py
# ‚Üí Choisir option 3 (simulation)
# ‚Üí V√©rifier ce qui sera supprim√©

# 4. Nettoyage r√©el (si simulation OK)
python scripts/cleanup_sqlite_buffer.py
# ‚Üí Choisir option 1 (nettoyer par jours)
# ‚Üí Confirmer
```

---

## üéì Principes Appliqu√©s

### OOP (Object-Oriented Programming)

**Bien que le script soit proc√©dural**, les principes OOP sont respect√©s :

- **Encapsulation** : Fonctions isol√©es avec responsabilit√©s claires
- **Abstraction** : `get_db_stats()` abstrait la complexit√© SQL
- **S√©paration des pr√©occupations** : Chaque fonction = une pr√©occupation

### SOLID

- ‚úÖ **S**ingle Responsibility : Chaque fonction = une responsabilit√©
- ‚úÖ **O**pen/Closed : Extensible via param√®tres (`target_date`, `dry_run`)
- ‚úÖ **L**iskov Substitution : N/A (pas d'h√©ritage)
- ‚úÖ **I**nterface Segregation : N/A (pas d'interfaces)
- ‚úÖ **D**ependency Inversion : Injection `db_path` (pas de hardcode)

### DRY

- ‚úÖ Pas de duplication de code SQL
- ‚úÖ Nettoyage tables li√©es factoris√©
- ‚úÖ Statistiques avant/apr√®s r√©utilisables

---

## üîê S√©curit√© et Garanties

### Garanties de Non-Perte de Donn√©es

1. **V√©rification Export Parquet** ‚úÖ
   - Bloque si Parquet absent
   - Bloque si Parquet corrompu

2. **Confirmation Utilisateur** ‚úÖ
   - Demande confirmation explicite
   - Permet annulation

3. **Mode Simulation** ‚úÖ
   - Permet test sans risque
   - Affiche ce qui sera supprim√©

4. **Statistiques Avant/Apr√®s** ‚úÖ
   - Transparence totale
   - V√©rification possible

### Recommandations Suppl√©mentaires

5. **Backup SQLite** (√† impl√©menter)
   - Cr√©er backup avant nettoyage
   - Permet rollback si probl√®me

6. **V√©rification Coh√©rence** (√† impl√©menter)
   - Comparer nombre lignes SQLite vs Parquet
   - V√©rifier dates coh√©rentes

---

## üìö R√©f√©rences

- **Code Source** : `scripts/cleanup_sqlite_buffer.py`
- **Export E1** : `src/e1/exporter.py` ‚Üí `export_all()`
- **Agr√©gation E1** : `src/e1/aggregator.py` ‚Üí `aggregate()`
- **Workflow Complet** : `docs/WORKFLOW_PARQUET_PYSPARK.md`

---

**Derni√®re mise √† jour** : 2025-12-20  
**Auteur** : DataSens Architecture Team  
**Niveau** : Senior Developer / Architect
