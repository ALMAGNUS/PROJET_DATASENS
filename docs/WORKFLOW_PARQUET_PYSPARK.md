# ğŸ“Š Workflow Parquet GOLD â†” PySpark - Guide Complet

## ğŸ¯ Vue d'ensemble

Ce document explique le workflow complet entre :
- **SQLite Database** (`datasens.db`) - **BUFFER TEMPORAIRE** E1 (collecte quotidienne)
- **Fichiers Parquet GOLD** - **STOCKAGE PERMANENT** (aprÃ¨s export depuis buffer)
- **PySpark** - Consommation Big Data depuis Parquet GOLD (E2/E3)

---

## ğŸ”„ Workflow Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              E1 PIPELINE - BUFFER SQLite (TEMPORAIRE)            â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   RAW Zone   â”‚â”€â”€â”€â–¶â”‚  SILVER Zone â”‚â”€â”€â”€â–¶â”‚  GOLD Zone   â”‚       â”‚
â”‚  â”‚  (SQLite)    â”‚    â”‚  (SQLite)    â”‚    â”‚  (SQLite)    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚                    â”‚                    â”‚                 â”‚
â”‚       â”‚                    â”‚                    â”‚                 â”‚
â”‚       â”‚                    â”‚                    â–¼                 â”‚
â”‚       â”‚                    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚       â”‚                    â”‚         â”‚  EXPORT PARQUET     â”‚     â”‚
â”‚       â”‚                    â”‚         â”‚  data/gold/         â”‚     â”‚
â”‚       â”‚                    â”‚         â”‚  date=YYYY-MM-DD/   â”‚     â”‚
â”‚       â”‚                    â”‚         â”‚  articles.parquet   â”‚     â”‚
â”‚       â”‚                    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚                    â”‚                    â”‚                 â”‚
â”‚       â”‚                    â”‚                    â”‚                 â”‚
â”‚       â–¼                    â–¼                    â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚    SQLite BUFFER (datasens.db) - TEMPORAIRE           â”‚        â”‚
â”‚  â”‚  âš ï¸ BUFFER: Collecte quotidienne                      â”‚        â”‚
â”‚  â”‚  - raw_data (articles bruts)                          â”‚        â”‚
â”‚  â”‚  - document_topic (topics)                            â”‚        â”‚
â”‚  â”‚  - model_output (sentiment)                          â”‚        â”‚
â”‚  â”‚                                                        â”‚        â”‚
â”‚  â”‚  ğŸ”„ AprÃ¨s export Parquet â†’ Peut Ãªtre vidÃ©/nettoyÃ©    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ EXPORT (E1 Pipeline)
                              â”‚ Buffer â†’ Stockage Permanent
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FICHIERS PARQUET GOLD (STOCKAGE PERMANENT)              â”‚
â”‚                                                                   â”‚
â”‚  data/gold/                                                      â”‚
â”‚  â”œâ”€â”€ date=2025-12-16/                                           â”‚
â”‚  â”‚   â””â”€â”€ articles.parquet  (216 lignes)                         â”‚
â”‚  â”œâ”€â”€ date=2025-12-18/                                           â”‚
â”‚  â”‚   â””â”€â”€ articles.parquet  (2,094 lignes)                       â”‚
â”‚  â”œâ”€â”€ date=2025-12-19/                                           â”‚
â”‚  â”‚   â””â”€â”€ articles.parquet  (42,466 lignes)                      â”‚
â”‚  â””â”€â”€ date=2025-12-20/                                           â”‚
â”‚       â””â”€â”€ articles.parquet  (43,131 lignes)                     â”‚
â”‚                                                                   â”‚
â”‚  âœ… STOCKAGE PERMANENT:                                          â”‚
â”‚     - Export depuis buffer SQLite                               â”‚
â”‚     - Fichiers immutables (une fois crÃ©Ã©s, ne changent pas)     â”‚
â”‚     - Chaque jour = nouveau fichier Parquet                     â”‚
â”‚     - Les fichiers restent sur le disque (pas de suppression)   â”‚
â”‚     - Source de vÃ©ritÃ© pour PySpark                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ LECTURE (PySpark E2/E3)
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PYSPARK (E2/E3)                               â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  GoldParquetReader                                    â”‚       â”‚
â”‚  â”‚  - Lit les fichiers Parquet GOLD                     â”‚       â”‚
â”‚  â”‚  - Mode LECTURE SEULE (isolation E1)                 â”‚       â”‚
â”‚  â”‚  - Pas de modification des fichiers                  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                              â”‚                                    â”‚
â”‚                              â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  GoldDataProcessor                                    â”‚       â”‚
â”‚  â”‚  - AgrÃ©gations                                        â”‚       â”‚
â”‚  â”‚  - Analyses Big Data                                  â”‚       â”‚
â”‚  â”‚  - Statistiques                                       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                              â”‚                                    â”‚
â”‚                              â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  E2 API Endpoints                                     â”‚       â”‚
â”‚  â”‚  - /api/v1/analytics/sentiment/distribution          â”‚       â”‚
â”‚  â”‚  - /api/v1/analytics/source/aggregation              â”‚       â”‚
â”‚  â”‚  - /api/v1/analytics/statistics                      â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Buffer SQLite vs Stockage Permanent Parquet

### Buffer SQLite (`datasens.db`) - TEMPORAIRE

**SQLite est un BUFFER** qui collecte les donnÃ©es quotidiennement :

Les zones RAW/SILVER/GOLD dans SQLite sont des **concepts logiques** utilisÃ©s par l'agrÃ©gateur E1 :

| Zone | Tables SQLite | Description |
|------|---------------|-------------|
| **RAW** | `raw_data` + `source` | Articles bruts directement depuis les extracteurs |
| **SILVER** | RAW + `document_topic` + `topic` | Articles nettoyÃ©s avec topics assignÃ©s |
| **GOLD** | SILVER + `model_output` | Articles enrichis avec topics + sentiment |

**Important** : Ces zones sont des **vues logiques** crÃ©Ã©es par `DataAggregator` :
- `aggregate_raw()` : Joint `raw_data` + `source`
- `aggregate_silver()` : Joint RAW + `document_topic` + `topic`
- `aggregate()` : Joint SILVER + `model_output`

**RÃ´le du Buffer SQLite** :
- âœ… Collecte quotidienne des articles depuis les sources
- âœ… Enrichissement (topics + sentiment)
- âœ… Export vers Parquet GOLD (stockage permanent)
- âš ï¸ **Peut Ãªtre vidÃ©/nettoyÃ© aprÃ¨s export** (donnÃ©es sauvegardÃ©es dans Parquet)

### Fichiers Parquet GOLD (Stockage Permanent)

Les fichiers Parquet sont le **STOCKAGE PERMANENT** crÃ©Ã©s par `GoldExporter` depuis le buffer SQLite :

```
data/gold/date=YYYY-MM-DD/articles.parquet
```

**Contenu** : Toutes les colonnes GOLD (RAW + SILVER + GOLD) dans un seul fichier Parquet.

---

## ğŸ”„ Processus d'Export E1 â†’ Parquet

### Ã‰tape 1 : ExÃ©cution Pipeline E1

```bash
python main.py
```

**Ce qui se passe** :
1. **Extraction** : Collecte depuis 14 sources â†’ `raw_data` (SQLite)
2. **Nettoyage** : QualitÃ©, dÃ©duplication â†’ `raw_data` (SQLite)
3. **Enrichissement Topics** : Assignation topics â†’ `document_topic` (SQLite)
4. **Enrichissement Sentiment** : Analyse sentiment â†’ `model_output` (SQLite)

### Ã‰tape 2 : AgrÃ©gation GOLD

```python
# Dans src/e1/pipeline.py
aggregator = DataAggregator(db_path)
df_gold = aggregator.aggregate()  # Joint toutes les tables
```

**Ce qui se passe** :
- `aggregate()` fait des JOINs SQL :
  ```sql
  SELECT 
    r.*,                    -- RAW (raw_data)
    t1.name as topic_1,     -- SILVER (document_topic + topic)
    t2.name as topic_2,     -- SILVER
    mo.label as sentiment,  -- GOLD (model_output)
    mo.score as sentiment_score
  FROM raw_data r
  LEFT JOIN document_topic dt1 ON r.raw_data_id = dt1.raw_data_id
  LEFT JOIN topic t1 ON dt1.topic_id = t1.topic_id
  LEFT JOIN model_output mo ON r.raw_data_id = mo.raw_data_id
  WHERE mo.model_name = 'sentiment_keyword'
  ```

### Ã‰tape 3 : Export Parquet

```python
# Dans src/e1/pipeline.py
exporter = GoldExporter()
result = exporter.export_all(df_gold, date.today())
```

**Ce qui se passe** :
- CrÃ©e le dossier : `data/gold/date=2025-12-20/`
- Exporte en Parquet : `data/gold/date=2025-12-20/articles.parquet`
- Exporte aussi en CSV : `exports/gold.csv` (pour rÃ©fÃ©rence)

**Code source** (`src/e1/exporter.py`) :
```python
def export_all(self, df: pd.DataFrame, partition_date: date | None = None) -> dict:
    d = partition_date or date.today()
    
    # Partitionnement par date
    p_path = self.base_dir / f"date={d:%Y-%m-%d}"
    p_path.mkdir(parents=True, exist_ok=True)
    
    # Export Parquet
    parquet = p_path / 'articles.parquet'
    df.to_parquet(parquet, index=False, engine='pyarrow')
    
    return {'parquet': parquet, 'rows': len(df)}
```

---

## ğŸ“¥ Processus de Lecture PySpark â† Parquet

### Comment PySpark rÃ©cupÃ¨re les Parquet

PySpark **NE MODIFIE JAMAIS** les fichiers Parquet. Il les lit en **lecture seule**.

#### 1. GoldParquetReader

```python
# Dans src/spark/adapters/gold_parquet_reader.py
reader = GoldParquetReader()

# Lire toutes les dates
df = reader.read_gold()

# Lire une date spÃ©cifique
df = reader.read_gold(date=date(2025, 12, 20))

# Lire une plage de dates
df = reader.read_gold_date_range(
    date(2025, 12, 18),
    date(2025, 12, 20)
)
```

**Ce qui se passe** :
1. Liste les partitions : `data/gold/date=*/articles.parquet`
2. Lit chaque fichier Parquet avec `spark.read.parquet()`
3. Unionne les DataFrames si plusieurs dates
4. Retourne un DataFrame Spark

**Code source** :
```python
def read_gold(self, date: date_type | None = None) -> DataFrame:
    if date:
        # Lecture date spÃ©cifique
        partition_path = self.base_path / f"date={date:%Y-%m-%d}" / "articles.parquet"
        return self.read(str(partition_path))
    else:
        # Lecture toutes les dates
        partitions = list(self.base_path.glob("date=*/articles.parquet"))
        # Lit et unionne toutes les partitions
        ...
```

#### 2. Isolation E1

**Principe** : PySpark ne touche JAMAIS Ã  SQLite. Il lit uniquement les Parquet.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SQLite (datasens.db)            â”‚
â”‚  âœ… ProtÃ©gÃ© - Pas d'accÃ¨s PySpark       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ EXPORT (E1 uniquement)
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Parquet GOLD (data/gold/)             â”‚
â”‚  âœ… Lecture seule PySpark                â”‚
â”‚  âœ… Pas de modification                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ LECTURE (PySpark)
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PySpark (E2/E3)                 â”‚
â”‚  âœ… Analyse Big Data                     â”‚
â”‚  âœ… AgrÃ©gations                          â”‚
â”‚  âœ… API Endpoints                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Concept de Buffer SQLite et Stockage Permanent Parquet

### âœ… SQLite = BUFFER TEMPORAIRE

**Clarification importante** :
- **SQLite (`datasens.db`)** = **BUFFER** qui collecte les donnÃ©es quotidiennement
- Les donnÃ©es sont **exportÃ©es** vers Parquet GOLD (stockage permanent)
- Le buffer SQLite **PEUT Ãªtre vidÃ©/nettoyÃ©** aprÃ¨s export
- Les donnÃ©es sont **sauvegardÃ©es** dans les fichiers Parquet avant nettoyage

### âœ… Parquet GOLD = STOCKAGE PERMANENT

**Les fichiers Parquet** :
- **Sont le stockage permanent** (export depuis le buffer SQLite)
- **NE SONT PAS** supprimÃ©s aprÃ¨s lecture par PySpark
- **RESTENT** sur le disque indÃ©finiment
- **Source de vÃ©ritÃ©** pour PySpark et analyses

### âœ… Workflow Quotidien

#### 1. Collecte Quotidienne dans Buffer SQLite

Chaque jour, E1 collecte les donnÃ©es dans le **buffer SQLite** :
- Extraction depuis 14 sources â†’ `raw_data` (SQLite)
- Enrichissement topics â†’ `document_topic` (SQLite)
- Enrichissement sentiment â†’ `model_output` (SQLite)

#### 2. Export Quotidien Buffer â†’ Parquet GOLD

Chaque jour, E1 **exporte** le buffer SQLite vers Parquet GOLD :

```
Buffer SQLite (datasens.db)
    â†“ EXPORT
data/gold/date=2025-12-20/articles.parquet  (43,131 lignes)
```

**Chaque fichier Parquet contient** :
- Tous les articles du buffer SQLite exportÃ©s ce jour-lÃ 
- Les fichiers plus rÃ©cents peuvent contenir plus d'articles (cumul si buffer non vidÃ©)

#### 3. Nettoyage Buffer SQLite (Optionnel)

**AprÃ¨s export vers Parquet**, le buffer SQLite peut Ãªtre nettoyÃ© :
- Les donnÃ©es sont **sauvegardÃ©es** dans Parquet avant nettoyage
- Le buffer peut Ãªtre vidÃ© pour libÃ©rer de l'espace
- Les donnÃ©es restent disponibles dans les fichiers Parquet

#### 4. Lecture PySpark depuis Parquet GOLD

PySpark lit depuis le **stockage permanent Parquet** :
- **Une date spÃ©cifique** : `reader.read_gold(date=date(2025, 12, 20))`
- **Toutes les dates** : `reader.read_gold()` (unionne tous les fichiers)
- **Une plage de dates** : `reader.read_gold_date_range(...)`

**PySpark ne lit JAMAIS directement depuis SQLite** (isolation E1)

---

## ğŸ—‘ï¸ Gestion du Cycle de Vie : Buffer SQLite et Parquet GOLD

### Nettoyage Buffer SQLite (aprÃ¨s export)

**AprÃ¨s export vers Parquet**, vous pouvez nettoyer le buffer SQLite :

```python
# scripts/cleanup_sqlite_buffer.py
import sqlite3
from pathlib import Path

def cleanup_sqlite_buffer(db_path: str, keep_days: int = 7):
    """
    Nettoie le buffer SQLite aprÃ¨s export Parquet
    
    âš ï¸ ATTENTION: Ne nettoyer QUE aprÃ¨s export Parquet rÃ©ussi
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Exemple: Supprimer les articles plus anciens que X jours
    # (Ã  adapter selon vos besoins)
    cursor.execute("""
        DELETE FROM raw_data 
        WHERE collected_at < date('now', '-' || ? || ' days')
    """, (keep_days,))
    
    # Nettoyer aussi les tables liÃ©es
    cursor.execute("DELETE FROM document_topic WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
    cursor.execute("DELETE FROM model_output WHERE raw_data_id NOT IN (SELECT raw_data_id FROM raw_data)")
    
    conn.commit()
    conn.close()
    print(f"Buffer SQLite nettoye (articles > {keep_days} jours supprimes)")
```

### Parquet GOLD = Stockage Permanent

**Les fichiers Parquet GOLD ne doivent PAS Ãªtre supprimÃ©s** (sauf si vraiment nÃ©cessaire) :
- Ils sont le stockage permanent
- PySpark en dÃ©pend pour les analyses
- Ils servent d'historique

### Script de Nettoyage Parquet (si vraiment nÃ©cessaire)

```python
# scripts/cleanup_old_parquet.py
from pathlib import Path
from datetime import date, timedelta

def cleanup_old_parquet(days_to_keep: int = 90):
    """
    âš ï¸ ATTENTION: Supprime les fichiers Parquet anciens
    Utiliser avec prÃ©caution - les Parquet sont le stockage permanent
    """
    gold_path = Path("data/gold")
    cutoff_date = date.today() - timedelta(days=days_to_keep)
    
    for partition_dir in gold_path.glob("date=*"):
        date_str = partition_dir.name.split("=")[1]
        partition_date = date.fromisoformat(date_str)
        
        if partition_date < cutoff_date:
            print(f"Suppression: {partition_dir}")
            # import shutil
            # shutil.rmtree(partition_dir)  # DÃ©commenter pour exÃ©cuter
```

---

## ğŸ“– Comment RÃ©cupÃ©rer les Parquet

### Option 1 : Via PySpark (RecommandÃ©)

```python
from spark.adapters import GoldParquetReader
from datetime import date

reader = GoldParquetReader()

# Lire toutes les dates
df = reader.read_gold()

# Lire une date spÃ©cifique
df = reader.read_gold(date=date(2025, 12, 20))

# Lire une plage
df = reader.read_gold_date_range(
    date(2025, 12, 18),
    date(2025, 12, 20)
)
```

### Option 2 : Via Script Interactif

```bash
python scripts/manage_parquet.py
# Option 2: Lire Parquet (toutes dates)
# Option 3: Lire Parquet (date spÃ©cifique)
```

### Option 3 : Via API E2

```bash
# Liste des dates disponibles
GET /api/v1/analytics/available-dates

# Statistiques (lit Parquet en arriÃ¨re-plan)
GET /api/v1/analytics/statistics?target_date=2025-12-20
```

### Option 4 : Directement avec PyArrow (sans Spark)

```python
import pyarrow.parquet as pq
import pandas as pd

# Lire un fichier Parquet directement
df = pd.read_parquet("data/gold/date=2025-12-20/articles.parquet")
print(df.head())
```

---

## ğŸ” DiffÃ©rence : SQLite vs Parquet

| Aspect | SQLite (datasens.db) | Parquet GOLD |
|--------|---------------------|--------------|
| **Format** | Base de donnÃ©es relationnelle | Fichiers colonnaires |
| **Structure** | Tables normalisÃ©es (raw_data, document_topic, model_output) | Fichier dÃ©normalisÃ© (toutes colonnes) |
| **AccÃ¨s** | SQL queries | Lecture via PySpark/PyArrow |
| **Modification** | âœ… CRUD complet | âŒ Lecture seule |
| **Partitionnement** | Par tables | Par date (`date=YYYY-MM-DD/`) |
| **Performance** | OptimisÃ© pour transactions | OptimisÃ© pour analytics Big Data |
| **Taille** | ~72 MB (43,022 articles) | ~87,907 lignes rÃ©parties sur 4 fichiers |

---

## ğŸ“Š Exemple Complet : Workflow JournÃ©e

### Jour 1 (2025-12-20)

**1. ExÃ©cution E1 Pipeline** :
```bash
python main.py
```

**RÃ©sultat** :
- SQLite : 43,022 articles (cumulÃ©)
- Export Parquet : `data/gold/date=2025-12-20/articles.parquet` (43,131 lignes)

**2. Lecture PySpark** :
```python
reader = GoldParquetReader()
df = reader.read_gold(date=date(2025, 12, 20))
# df contient 43,131 lignes
```

**3. Analyse** :
```python
processor = GoldDataProcessor()
stats = processor.get_statistics(df)
# Analyse Big Data sur les 43,131 lignes
```

### Jour 2 (2025-12-21)

**1. ExÃ©cution E1 Pipeline** :
```bash
python main.py
```

**RÃ©sultat** :
- SQLite : 43,500 articles (nouveaux articles ajoutÃ©s)
- Export Parquet : `data/gold/date=2025-12-21/articles.parquet` (43,500 lignes)

**2. Lecture PySpark** :
```python
# Lire seulement le nouveau fichier
df_new = reader.read_gold(date=date(2025, 12, 21))

# OU lire toutes les dates (cumul)
df_all = reader.read_gold()  # Unionne date=2025-12-20 + date=2025-12-21
```

**3. Les anciens fichiers restent** :
- `data/gold/date=2025-12-20/articles.parquet` âœ… Toujours prÃ©sent
- `data/gold/date=2025-12-21/articles.parquet` âœ… Nouveau fichier

---

## âœ… RÃ©sumÃ©

1. **SQLite (`datasens.db`)** : **BUFFER TEMPORAIRE** E1
   - Collecte quotidienne des articles
   - Zones RAW/SILVER/GOLD sont des vues logiques
   - Peut Ãªtre vidÃ©/nettoyÃ© aprÃ¨s export Parquet

2. **Export Parquet GOLD** : **STOCKAGE PERMANENT**
   - CrÃ©Ã© quotidiennement depuis le buffer SQLite
   - PartitionnÃ© par date (`date=YYYY-MM-DD/`)
   - Fichiers immutables (ne changent jamais)

3. **PySpark** : Lit les Parquet en lecture seule
   - Ne lit JAMAIS directement SQLite (isolation E1)
   - Utilise uniquement les fichiers Parquet GOLD
   - Ne modifie jamais les fichiers Parquet

4. **Workflow** : Buffer SQLite â†’ Export Parquet â†’ Stockage Permanent â†’ PySpark
   - Buffer SQLite peut Ãªtre nettoyÃ© aprÃ¨s export
   - Parquet GOLD reste sur le disque (stockage permanent)
   - PySpark consomme depuis Parquet (pas depuis SQLite)

---

## ğŸ”— Fichiers ClÃ©s

- **Export E1** : `src/e1/exporter.py` â†’ `export_all()`
- **AgrÃ©gation E1** : `src/e1/aggregator.py` â†’ `aggregate()`
- **Lecture PySpark** : `src/spark/adapters/gold_parquet_reader.py` â†’ `read_gold()`
- **Traitement PySpark** : `src/spark/processors/gold_processor.py`

---

**DerniÃ¨re mise Ã  jour** : 2025-12-20
