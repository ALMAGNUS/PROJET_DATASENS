# ğŸ“Š DataSens E1 â€” Data Extraction & Transformation Pipeline

![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen?style=flat-square)
![Python](https://img.shields.io/badge/Python-3.10+-blue?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-blue?style=flat-square)
![E1 Complete](https://img.shields.io/badge/E1-v1.0.0%20Complete-green?style=flat-square)

---

```text
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â•‘
â•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•     â•‘
â•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â•‘
â•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘     â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•‘
â•‘     â•šâ•â•â•â•â•â•  â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•     â•‘
â•‘                                                                              â•‘
â•‘                   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                     â•‘
â•‘                   â•‘  ---------- README -----------     â•‘                     â•‘
â•‘                   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â•‘
â•‘                                                                              â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## ğŸ¯ Overview

**DATASENS E1** is a professional data extraction, transformation, and export pipeline that:

- **Extracts** from **10 heterogeneous sources** (RSS, APIs, web scraping)
- **Cleans & standardizes** with quality scoring and deduplication  
- **Exports** to three data zones (RAW â†’ SILVER â†’ GOLD)
- **Produces** production-ready parquet files for E2/E3 ML pipelines

**216 articles | 0 corruption | Zero duplicates | 100% clean**

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

**Core Dependencies:**
- `pandas==2.3.3` â€” Data processing
- `pyarrow==22.0.0` â€” Parquet engine
- `kagglehub==0.2.5` â€” Kaggle datasets API
- `feedparser` â€” RSS extraction
- `requests`, `beautifulsoup4` â€” HTTP & web scraping
- `sqlalchemy` â€” Database ORM

### 2. Initialize Database

```bash
python scripts/setup_with_sql.py
```

Creates SQLite database with:
- 7 tables (source, raw_data, sync_log, topic, document_topic, model_output)
- 10 sources configured
- Proper indexes for performance

### 3. Run E1 Pipeline

```bash
python main.py
```

**Output:**
- Articles extracted to database
- sync_log updated (10 sources logged)
- **Export RAW/SILVER/GOLD inclus** dans le pipeline

### 4. Export ou rÃ©gÃ©nÃ©ration (optionnel)

L'export est dÃ©jÃ  effectuÃ© par `main.py`. Pour rÃ©gÃ©nÃ©rer manuellement les exports :

```bash
python scripts/regenerate_exports.py
```

**Produces:**
- ğŸ”´ `data/raw/sources_2025-12-16/` (JSON + CSV)
- ğŸŸ¡ `data/silver/v_2025-12-16/` (Parquet)
- ğŸŸ¢ `data/gold/date=2025-12-16/` (PySpark parquet)

---

## ğŸ“ Three-Zone Architecture

```
ğŸ”´ RAW ZONE (Native Formats - NO Processing)
   data/raw/sources_2025-12-16/
   â”œâ”€ raw_articles.json (137.4 KB) â† Direct from extractors
   â””â”€ raw_articles.csv  (100.4 KB)  â† No transformations

ğŸŸ¡ SILVER ZONE (Cleaned & Standardized)
   data/silver/v_2025-12-16/
   â””â”€ silver_articles.parquet (64.5 KB)
      â€¢ Deduplicated (fingerprint-based)
      â€¢ Quality scores (0-1 scale)
      â€¢ Topic tagging (8 topics, multiple per article)
      â€¢ Text cleaning indicators

ğŸŸ¢ GOLD ZONE (ML-Enriched, PySpark Ready)
   data/gold/date=2025-12-16/
   â””â”€ articles.parquet (67.9 KB)
      â€¢ Sentiment analysis (176 neutral, 32 positive, 8 negative)
      â€¢ Confidence scores (0-1)
      â€¢ Processing metadata
      â€¢ Partitioned for PySpark
```

---

## ğŸ“š Core Files

| File | Purpose | Status |
|------|---------|--------|
| **main.py** | E1 pipeline orchestration (inclut export RAW/SILVER/GOLD) | âœ… |
| **scripts/setup_with_sql.py** | Database initialization | âœ… |
| **scripts/regenerate_exports.py** | RÃ©gÃ©nÃ©ration manuelle RAW â†’ SILVER â†’ GOLD | âœ… |
| **src/e1/core.py** | Extracteurs et transformers E1 | âœ… |
| **sources_config.json** | 10 sources configuration | âœ… |
| **requirements.txt** | All dependencies | âœ… |

> **ZZDB MongoDB** : pour activer la source `zzdb_synthetic`, installer `pymongo`
> (sinon la validation ZZDB affichera un warning).

---

## ğŸ”— Data Sources (15 sources actives)

### Sources Actives (15 sources)

> **Note**: Les statistiques ci-dessous sont une **photo au 2025-12-20**. La collecte Ã©volue quotidiennement pour les sources dynamiques. Les nombres d'articles augmentent Ã  chaque exÃ©cution du pipeline.

| # | Source | Type | Records (20/12/2025) | Status |
|---|--------|------|---------------------|--------|
| 1 | kaggle_french_opinions | Dataset | 38,327 | âœ“ Fondation |
| 2 | google_news_rss | RSS | 1,456 | âœ“ Dynamique |
| 3 | zzdb_csv | CSV | 930 | âœ“ Fondation |
| 4 | trustpilot_reviews | Scraping | 658 | âœ“ Dynamique |
| 5 | yahoo_finance | RSS | 624 | âœ“ Dynamique |
| 6 | reddit_france | API | 377 | âœ“ Dynamique |
| 7 | rss_french_news | RSS | 259 | âœ“ Dynamique |
| 8 | openweather_api | API | 187 | âœ“ Dynamique |
| 9 | gdelt_events | BigData | 79 | âœ“ Fondation |
| 10 | datagouv_datasets | Dataset | 50 | âœ“ Dynamique |
| 11 | ifop_barometers | Scraping | 18 | âœ“ Dynamique |
| 12 | insee_indicators | API | 5 | âœ“ Dynamique |
| 13 | agora_consultations | API | n/a | âœ“ Dynamique |
| 14 | GDELT_Last15_English | BigData | 2 | âœ“ Dynamique |
| 15 | GDELT_Master_List | BigData | 0 | âœ“ Dynamique |

**Total articles en base** (au 20/12/2025): **43,022 articles**

**Classification**:
- **Fondation** (statiques, intÃ©grÃ©es une fois) : `kaggle_french_opinions`, `gdelt_events`, `zzdb_csv`
  - Ces sources sont figÃ©es aprÃ¨s leur premiÃ¨re intÃ©gration et ne sont plus collectÃ©es
- **Dynamiques** (collecte quotidienne) : Toutes les autres sources actives
  - Les sources dynamiques collectent de nouveaux articles Ã  chaque exÃ©cution du pipeline
  - Les nombres d'articles augmentent quotidiennement pour ces sources

### Sources Inactives (pour rÃ©fÃ©rence)

| Source | Type | Status |
|--------|------|--------|
| Kaggle_StopWords_28Lang | Dataset | Inactif |
| Kaggle_StopWords | Dataset | Inactif |
| Kaggle_FrenchFinNews | Dataset | Inactif |
| Kaggle_SentimentLexicons | Dataset | Inactif |
| Kaggle_InsuranceReviews | Dataset | Inactif |
| Kaggle_FrenchTweets | Dataset | Inactif |
| monavis_citoyen | Scraping | Inactif |
| zzdb_synthetic | MongoDB | Inactif |

---

## ğŸ“Š Dashboard de Visualisation

### âœ… Automatique et Dynamique

Le systÃ¨me inclut **3 outils de visualisation** qui se mettent Ã  jour automatiquement :

1. **Rapport de Collecte** : S'affiche automatiquement aprÃ¨s chaque `python main.py`
2. **Dashboard Global** : Vue d'ensemble complÃ¨te avec `python show_dashboard.py`
3. **Visualiseur CSV** : Explorer les fichiers exports/ avec `python view_exports.py`

### ğŸ“‹ Rapport de Collecte (Session Actuelle)

Le rapport s'affiche **automatiquement** aprÃ¨s chaque collecte et montre :
- Articles collectÃ©s, taggÃ©s, analysÃ©s dans cette session
- DÃ©tail par source des nouveaux articles
- Distribution topics et sentiment des nouveaux articles

### ğŸ“Š Dashboard Global

```bash
# Afficher le dashboard complet (toujours Ã  jour)
python scripts/show_dashboard.py
```

Affiche :
- **RÃ©sumÃ© global** : Total articles, uniques, nouveaux aujourd'hui, enrichis
- **Nouveaux articles** : DÃ©tail par source des articles collectÃ©s aujourd'hui
- **Enrichissement Topics** : Articles taggÃ©s, topics utilisÃ©s, confiance moyenne
- **Enrichissement Sentiment** : Distribution positif/neutre/nÃ©gatif
- **Articles par source** : Statistiques dÃ©taillÃ©es par source
- **Ã‰valuation IA** : Status du dataset pour l'entraÃ®nement IA

### ğŸ‘€ Visualiser les CSV dans exports/

```bash
# Script interactif pour explorer les CSV
python scripts/view_exports.py
```

Les fichiers CSV sont aussi directement accessibles dans `exports/` :
- **`raw.csv`** : DonnÃ©es brutes (DB + fichiers locaux)
- **`silver.csv`** : DonnÃ©es nettoyÃ©es avec topics
- **`gold.csv`** : DonnÃ©es complÃ¨tes avec topics + sentiment

Vous pouvez les ouvrir directement dans Excel, Notepad, ou les importer dans Power BI.

### ğŸ”„ Enrichir rÃ©troactivement tous les articles

```bash
# Enrichir tous les articles existants (topics + sentiment)
python scripts/enrich_all_articles.py
```

ğŸ“– **Guide complet** : Voir `docs/DASHBOARD_GUIDE.md` pour plus de dÃ©tails

## ğŸ“Š Data Quality Verification

### Pipeline Summary âœ“

- **Total Articles Extracted**: 81
- **Articles Cleaned**: 81
- **Articles Loaded to DB**: 10 (new today)
- **Total in DB**: 1017 (consolidated across runs)
- **Deduplication Rate**: 87.7%
- **Quality Score**: 100%

### Export Outputs âœ“

| File | Records | Size | Format |
|------|---------|------|--------|
| raw.csv | 1017 | 0.73 MB | CSV (raw data + Kaggle) |
| silver.csv | 1017 | 0.17 MB | CSV (classified by DOCUMENT_TOPIC) |
| gold.csv | 9 | 1 KB | CSV (aggregated by source) |
| gold.parquet | 9 | 10 KB | Parquet (for Power BI) |

### Transformations Applied âœ“

| Transformation | Coverage | Status |
|---|---|---|
| Deduplication | 71/81 | âœ“ |
| Quality Scoring | 1017/1017 | âœ“ |
| Topic Classification | 1017/1017 (8 topics) | âœ“ |
| Sentiment Analysis | 9 sources (MODEL_OUTPUT) | âœ“ |
| Partitioned Raw Data | sources_2025-12-17 | âœ“ |

---

## ğŸ—ï¸ Database Schema

### 6 Core Tables (E1)

1. **source** â€” 10 configured sources
2. **raw_data** â€” 216 articles (direct from extractors)
3. **sync_log** â€” 20 logs (2 runs Ã— 10 sources)
4. **topic** â€” 8 predefined topics
5. **document_topic** â€” 207 article-topic mappings
6. **model_output** â€” 648 ML predictions

**Note:** `sqlite_sequence` est une table systÃ¨me SQLite (gÃ©rÃ©e automatiquement) qui stocke les compteurs AUTOINCREMENT. Elle n'est pas une table mÃ©tier.

---

## ğŸ“ˆ Statistics

| Metric | Value |
|--------|-------|
| Total Articles | 216 |
| Active Sources | 10/10 |
| Topics Created | 8 |
| Duplicates Removed | 0 |
| Data Corruption | 0 |
| Quality Score Range | 0.01 - 0.99 |
| Model Outputs | 648 |
| Transformations | 100% complete |

---

## âœ… Production Readiness

âœ… **Code Quality**
- Ruff linting: 100% pass
- Type hints throughout
- OOP architecture (SOLID principles)
- No hardcoded values

âœ… **Data Quality**
- Zero duplicates
- Zero corruption
- 100% article coverage
- Proper deduplication

âœ… **Documentation**
- README (this file)
- AGILE_ROADMAP.md (43 user stories)
- SCHEMA_DESIGN.md (database design)
- CHANGELOG.md (version history)
- **ConformitÃ©** : Audits E1â€“E5, RGPD, OWASP, monitoring, incidents (tout bÃ¢chÃ©)
- **docs/** : 60+ documents (architecture, flux, audits, procÃ©dures)

âœ… **Dependencies**
- All listed in requirements.txt
- Pinned versions
- pandas, pyarrow, fastparquet installed

---

## ğŸ“ File Structure

```
PROJET_DATASENS/
â”œâ”€â”€ main.py                          # E1 orchestration (inclut export RAW/SILVER/GOLD)
â”œâ”€â”€ scripts/setup_with_sql.py        # Database setup
â”œâ”€â”€ requirements.txt                 # Dependencies
â”œâ”€â”€ sources_config.json              # Sources config
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ pytest.ini                       # Configuration pytest
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ e1/                          # E1 ISOLÃ‰ (package privÃ©)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core.py                  # Extracteurs et transformers
â”‚   â”‚   â”œâ”€â”€ repository.py            # Repository pattern
â”‚   â”‚   â”œâ”€â”€ tagger.py                # Topic tagger
â”‚   â”‚   â”œâ”€â”€ analyzer.py             # Sentiment analyzer
â”‚   â”‚   â”œâ”€â”€ aggregator.py            # Data aggregator
â”‚   â”‚   â”œâ”€â”€ exporter.py             # Gold exporter
â”‚   â”‚   â””â”€â”€ pipeline.py             # E1Pipeline isolÃ©
â”‚   â”œâ”€â”€ e2/                          # E2 (FastAPI + RBAC) - PRÃŠT
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ e3/                          # E3 (PySpark + ML) - PRÃŠT
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ shared/                      # INTERFACES (contrats E1 â†” E2/E3)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ interfaces.py           # E1DataReader (lecture seule)
â”‚   â”œâ”€â”€ dashboard.py                 # Dashboard utilitaires
â”‚   â”œâ”€â”€ collection_report.py         # Rapport de collecte
â”‚   â””â”€â”€ metrics.py                   # Prometheus metrics
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_e1_isolation.py         # Tests non-rÃ©gression E1
â”‚   â””â”€â”€ README_E1_ISOLATION.md       # Guide tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PLAN_ACTION_E1_E2_E3.md      # Plan d'action dÃ©taillÃ©
â”‚   â”œâ”€â”€ E1_ISOLATION_STRATEGY.md    # StratÃ©gie isolation
â”‚   â”œâ”€â”€ E1_ISOLATION_COMPLETE.md    # RÃ©capitulatif Phase 0
â”‚   â””â”€â”€ ROADMAP_EVOLUTION.md         # Roadmap E1 â†’ E2 â†’ E3
â””â”€â”€ data/
    â”œâ”€â”€ raw/
    â”‚   â””â”€â”€ sources_2025-12-20/
    â”‚       â”œâ”€â”€ raw_articles.json
    â”‚       â””â”€â”€ raw_articles.csv
    â”œâ”€â”€ silver/
    â”‚   â””â”€â”€ v_2025-12-20/
    â”‚       â””â”€â”€ silver_articles.parquet
    â””â”€â”€ gold/
        â””â”€â”€ date=2025-12-20/
            â””â”€â”€ articles.parquet
```

---

## ğŸ“ Key Concepts

### Three-Zone Architecture

- **RAW**: Unprocessed data directly from sources (JSON/CSV)
- **SILVER**: Cleaned, standardized, deduplicated (Parquet)
- **GOLD**: ML-enriched, production-ready (PySpark Parquet)

### Immutable Data Pipeline

1. No modifications to raw data (source of truth)
2. All transformations tracked
3. Lineage clearly documented
4. Easy to reprocess if needed

### PySpark Ready

- GOLD zone uses `date=2025-12-16/` partitioning
- Compatible with PySpark's partitioned dataset format
- Can be read with: `spark.read.parquet("data/gold/")`

---

## ğŸ”’ E1 Isolation (Phase 0)

Package `src/e1/` isolÃ©. E2/E3 lisent via `E1DataReader` uniquement â€” pas de touche au code E1. Tests non-rÃ©gression en place. DÃ©tails : `docs/E1_ISOLATION_COMPLETE.md`.

---

## ğŸ“‹ Doc & conformitÃ©

**Grilles E1â€“E5** : `AUDIT_E1_COMPETENCES.md` â€¦ `AUDIT_E5_COMPETENCES.md`, E4 = Ã©carts + plan. E1/E2/E3/E5 validÃ©s.

**RGPD / sÃ©cu** : Registre traitements, procÃ©dure tri DP, OWASP Top 10 (dans README_E2_API).

**Monitoring / incidents** : MÃ©triques, seuils, alertes, Prometheus/Grafana, accessibilitÃ©. ProcÃ©dure incidents prÃªte.

Index complet : `docs/README.md`.

---

## ğŸš€ Phases du Projet

**Phase 0** : E1 isolÃ©, `E1DataReader`, tests non-rÃ©gression.

**Phase 2** : FastAPI + RBAC, JWT, audit trail. Tests API OK.

**Phase 3** : PySpark (singleton local), GoldParquetReader, 4 endpoints analytics. ~88k lignes Parquet GOLD.

**Outils PySpark** :
```bash
# Shell interactif PySpark
python scripts/pyspark_shell.py

# Tests rapides locaux
python scripts/test_spark_simple.py

# Tests complets
pytest tests/test_spark_integration.py -v
```

Endpoints : `sentiment/distribution`, `source/aggregation`, `statistics`, `available-dates`.

Phase 4 : FlauBERT/CamemBERT fine-tuning, MLflow.  
Phase 5 : Dashboard Streamlit.  
Phase 6 : Mistral insights.  

Plan dÃ©taillÃ© : `docs/PLAN_ACTION_E1_E2_E3.md`

---

## ğŸ“„ License

MIT License â€” See LICENSE.md

---

## ğŸ¤ Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md)

---

---

## ğŸ“Š Parquet GOLD vs Base de DonnÃ©es

### DiffÃ©rence entre Parquet et Dashboard E1

Le **dashboard E1** affiche le **total des articles dans la base de donnÃ©es SQLite** (`datasens.db`), qui contient **tous les articles collectÃ©s depuis le dÃ©but** (43,022 articles au 20/12/2025).

Les **fichiers Parquet GOLD** sont **exportÃ©s par date** lors de chaque exÃ©cution du pipeline E1. Chaque fichier Parquet contient uniquement les articles **exportÃ©s pour cette date spÃ©cifique**.

**Fichiers Parquet disponibles** :
- `data/gold/date=2025-12-16/articles.parquet` : 216 lignes
- `data/gold/date=2025-12-18/articles.parquet` : 2,094 lignes
- `data/gold/date=2025-12-19/articles.parquet` : 42,466 lignes
- `data/gold/date=2025-12-20/articles.parquet` : 43,131 lignes

**Total Parquet** : 87,907 lignes (certains articles peuvent Ãªtre dans plusieurs fichiers si exportÃ©s plusieurs fois)

### Manipuler les Parquet avec PySpark

Utilisez le script interactif pour manipuler vos fichiers Parquet :

```bash
# Windows
scripts\manage_parquet.bat

# Linux/Mac
bash scripts/manage_parquet.sh
```

**FonctionnalitÃ©s disponibles** :
- âœ… Lire et afficher les donnÃ©es Parquet
- âœ… Filtrer les donnÃ©es (conditions SQL)
- âœ… Modifier les valeurs
- âœ… Ajouter des colonnes
- âœ… Supprimer des lignes
- âœ… Sauvegarder en nouveaux fichiers Parquet
- âœ… Appliquer des traitements (agrÃ©gations, statistiques)

**Exemple d'utilisation** :
1. Lancer le script : `python scripts/manage_parquet.py`
2. Choisir option `2` : Lire Parquet (toutes dates)
3. Choisir option `5` : Filtrer DataFrame (ex: `sentiment = 'positif'`)
4. Choisir option `9` : Sauvegarder en nouveau fichier Parquet

---

**Last Updated:** February 12, 2026  
**Version:** 1.5.0  
**Status:** Production Ready  
E1/E2/E3 bouclÃ©s. GoldAI merge opÃ©rationnel. Doc audit E1â€“E5 couverte.
