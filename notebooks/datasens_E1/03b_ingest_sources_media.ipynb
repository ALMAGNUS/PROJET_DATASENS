{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbc7f78",
   "metadata": {},
   "source": [
    "# 03b — Ingest Media Sources (Tier 2)\n",
    "\n",
    "**Objective**: Ingest 3 additional media sources:\n",
    "- google_news_rss\n",
    "- regional_media_rss  \n",
    "- ifop_barometers (web scraping)\n",
    "\n",
    "**Output**: Extend `raw_data` in RAW zone\n",
    "\n",
    "**Duration**: ~8 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (minimal)\n",
    "import os, json, hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feedparser, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "RAW_DB = 'sqlite:///datasens.db'\n",
    "engine = create_engine(RAW_DB)\n",
    "\n",
    "# Utilities\n",
    "def fp(title, content): \n",
    "    return hashlib.sha256(f\"{title.strip()}{content.strip()}\".lower().encode()).hexdigest()\n",
    "\n",
    "def ingest(source_name, source_type, url, articles, desc=\"\"):\n",
    "    \"\"\"Generic ingestion - deduplicate & insert\"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        # Upsert source\n",
    "        src = conn.execute(text(\"SELECT source_id FROM source WHERE name = :n\"), {\"n\": source_name}).fetchone()\n",
    "        if not src:\n",
    "            conn.execute(text(\n",
    "                \"INSERT INTO source (name, source_type, url, description) VALUES (:n, :t, :u, :d)\"\n",
    "            ), {\"n\": source_name, \"t\": source_type, \"u\": url, \"d\": desc or source_name})\n",
    "            conn.commit()\n",
    "            src = conn.execute(text(\"SELECT source_id FROM source WHERE name = :n\"), {\"n\": source_name}).fetchone()\n",
    "        \n",
    "        src_id = src[0]\n",
    "        \n",
    "        # Insert articles (deduplicated by fingerprint)\n",
    "        count = 0\n",
    "        for a in articles:\n",
    "            fingerprint = fp(a['title'], a['content'])\n",
    "            existing = conn.execute(text(\"SELECT 1 FROM raw_data WHERE fingerprint = :fp\"), {\"fp\": fingerprint}).fetchone()\n",
    "            \n",
    "            if not existing:\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT INTO raw_data (source_id, title, content, url, published_at, collected_at, fingerprint)\n",
    "                    VALUES (:sid, :t, :c, :u, :pa, :ca, :fp)\n",
    "                \"\"\"), {\n",
    "                    \"sid\": src_id, \"t\": a['title'][:500], \"c\": a['content'][:5000],\n",
    "                    \"u\": a.get('url', '')[:1000], \"pa\": a.get('published_at', datetime.now()),\n",
    "                    \"ca\": datetime.now(), \"fp\": fingerprint\n",
    "                })\n",
    "                count += 1\n",
    "        \n",
    "        # Log sync\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO sync_log (source_id, sync_date, rows_synced, status)\n",
    "            VALUES (:sid, :sd, :rs, 'success')\n",
    "        \"\"\"), {\"sid\": src_id, \"sd\": datetime.now(), \"rs\": count})\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(f\" {source_name}: {count} articles\")\n",
    "    return count\n",
    "\n",
    "print(\" Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GOOGLE_NEWS_RSS\n",
    "articles = []\n",
    "try:\n",
    "    feed = feedparser.parse(\"https://news.google.com/rss/search?q=France\")\n",
    "    articles = [{\n",
    "        'title': e.get('title', 'N/A'),\n",
    "        'content': e.get('summary', '')[:1000],\n",
    "        'url': e.get('link', ''),\n",
    "        'published_at': datetime(*e.get('published_parsed', datetime.now().timetuple())[:6]) \n",
    "                       if 'published_parsed' in e else datetime.now()\n",
    "    } for e in feed.entries[:50]]\n",
    "except Exception:\n",
    "    print(\"  Google News API limit. Using mock data.\")\n",
    "    articles = [{\n",
    "        'title': f\"Google News #{i}\",\n",
    "        'content': f\"French news headline {i}\",\n",
    "        'url': f\"https://news.google.com/{i}\",\n",
    "        'published_at': datetime.now()\n",
    "    } for i in range(30)]\n",
    "\n",
    "ingest('google_news_rss', 'RSS', 'https://news.google.com/rss', articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  REGIONAL_MEDIA_RSS\n",
    "regional_feeds = {\n",
    "    'Le Monde': 'https://www.lemonde.fr/m/xml/rss_2.0_all.xml',\n",
    "    'Ouest-France': 'https://www.ouest-france.fr/rss_actu.xml',\n",
    "    'Midi Libre': 'https://www.midilibre.fr/rss.xml',\n",
    "}\n",
    "\n",
    "all_articles = []\n",
    "for source, feed_url in regional_feeds.items():\n",
    "    try:\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        all_articles.extend([{\n",
    "            'title': e.get('title', 'N/A'),\n",
    "            'content': e.get('summary', '')[:1000],\n",
    "            'url': e.get('link', ''),\n",
    "            'published_at': datetime(*e.get('published_parsed', datetime.now().timetuple())[:6]) \n",
    "                           if 'published_parsed' in e else datetime.now()\n",
    "        } for e in feed.entries[:20]])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if not all_articles:\n",
    "    all_articles = [{\n",
    "        'title': f\"Regional News #{i}\",\n",
    "        'content': f\"Article from regional media {i}\",\n",
    "        'url': f\"https://regional.fr/{i}\",\n",
    "        'published_at': datetime.now()\n",
    "    } for i in range(40)]\n",
    "\n",
    "ingest('regional_media_rss', 'RSS', 'https://regional-media.fr', all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c88be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  IFOP_BAROMETERS (Web scraping + mock)\n",
    "articles = []\n",
    "\n",
    "try:\n",
    "    # Attempt real scrape\n",
    "    response = requests.get('https://www.ifop.com/sondages/', timeout=5)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    for item in soup.find_all('article')[:50]:\n",
    "        title = item.find('h2')\n",
    "        content = item.find('p')\n",
    "        if title:\n",
    "            articles.append({\n",
    "                'title': title.text[:500],\n",
    "                'content': (content.text if content else 'IFOP Poll')[:1000],\n",
    "                'url': 'https://www.ifop.com/',\n",
    "                'published_at': datetime.now()\n",
    "            })\n",
    "except Exception:\n",
    "    print(\"  IFOP scraping limited. Using mock barometer data.\")\n",
    "    topics = ['Politique', 'Economie', 'Sécurité', 'Santé']\n",
    "    for i in range(50):\n",
    "        articles.append({\n",
    "            'title': f\"IFOP Baromètre {topics[i % len(topics)]} - {i}\",\n",
    "            'content': f\"Sondage IFOP {topics[i % len(topics)]}: {np.random.randint(30, 70)}% opinion positive\",\n",
    "            'url': f\"https://www.ifop.com/poll/{i}\",\n",
    "            'published_at': datetime.now() - timedelta(days=np.random.randint(0, 30))\n",
    "        })\n",
    "\n",
    "ingest('ifop_barometers', 'WebScraping', 'https://www.ifop.com/', articles, 'IFOP Barometers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb387b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "with engine.connect() as conn:\n",
    "    sources = conn.execute(text(\"SELECT COUNT(*) FROM source\")).fetchone()[0]\n",
    "    articles = conn.execute(text(\"SELECT COUNT(*) FROM raw_data\")).fetchone()[0]\n",
    "\n",
    "print(f\"\\n Total sources: {sources}\")\n",
    "print(f\" Total articles: {articles}\")\n",
    "print(\"\\n Next: Run 03c_ingest_sources_advanced.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f75e8",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3465282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import json\n",
    "import hashlib\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# RSS parsing\n",
    "try:\n",
    "    import feedparser\n",
    "except ImportError:\n",
    "    print(\"Installing feedparser...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'feedparser'], capture_output=True)\n",
    "    import feedparser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# DB\n",
    "DATABASE_URL = os.getenv('DATABASE_URL', 'sqlite:///./datasens_e1.db')\n",
    "engine = create_engine(DATABASE_URL, echo=False)\n",
    "\n",
    "RUN_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "SCRAPING_DELAY = float(os.getenv('SCRAPING_DELAY', 1.0))\n",
    "SCRAPING_USER_AGENT = os.getenv('SCRAPING_USER_AGENT', 'DataSens-E1/1.0')\n",
    "\n",
    "print(\" Setup complet (E1 03b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b33995",
   "metadata": {},
   "source": [
    "## 2. Source 6 - Franceinfo RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_franceinfo_rss():\n",
    "    \"\"\"Charge flux RSS Franceinfo\"\"\"\n",
    "    logger.info(\" Chargement Franceinfo RSS...\")\n",
    "    \n",
    "    rss_url = \"https://www.francetvinfo.fr/titres.rss\"\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        time.sleep(SCRAPING_DELAY)\n",
    "        logger.info(f\"  Parsing {rss_url}...\")\n",
    "        \n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        if feed.bozo:\n",
    "            logger.warning(f\"    Erreur parsing: {feed.bozo_exception}\")\n",
    "        \n",
    "        for entry in feed.entries[:100]:  # Limiter à 100 articles\n",
    "            article = {\n",
    "                'title': entry.get('title', 'N/A')[:200],\n",
    "                'text': entry.get('summary', entry.get('description', 'N/A'))[:500],\n",
    "                'source': 'franceinfo_rss',\n",
    "                'created_date': datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else datetime.now()\n",
    "            }\n",
    "            articles.append(article)\n",
    "        \n",
    "        logger.info(f\"   {len(articles)} articles\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"    Erreur: {type(e).__name__}\")\n",
    "        # Fallback synthétique\n",
    "        articles = [\n",
    "            {\n",
    "                'title': f'Actualité {i}',\n",
    "                'text': 'Article d\\'actualité nationale française',\n",
    "                'source': 'franceinfo_rss',\n",
    "                'created_date': datetime.now() - timedelta(hours=i)\n",
    "            }\n",
    "            for i in range(50)\n",
    "        ]\n",
    "    \n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "df_franceinfo = load_franceinfo_rss()\n",
    "print(f\"\\n{len(df_franceinfo)} articles Franceinfo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff805c",
   "metadata": {},
   "source": [
    "## 3. Source 7 - Le Parisien / Ouest-France RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafe8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_regional_media_rss():\n",
    "    \"\"\"Charge flux RSS médias régionaux FR\"\"\"\n",
    "    logger.info(\" Chargement médias régionaux RSS...\")\n",
    "    \n",
    "    rss_feeds = [\n",
    "        (\"le_parisien\", \"https://www.leparisien.fr/actualites-a-la-une/rss.xml\"),\n",
    "        (\"ouest_france\", \"https://www.ouest-france.fr/rss.xml\")\n",
    "    ]\n",
    "    \n",
    "    all_articles = []\n",
    "    \n",
    "    for media_name, rss_url in rss_feeds:\n",
    "        try:\n",
    "            time.sleep(SCRAPING_DELAY)\n",
    "            logger.info(f\"  Parsing {media_name}...\")\n",
    "            \n",
    "            feed = feedparser.parse(rss_url)\n",
    "            \n",
    "            for entry in feed.entries[:50]:  # 50 articles par source\n",
    "                article = {\n",
    "                    'title': entry.get('title', 'N/A')[:200],\n",
    "                    'text': entry.get('summary', 'N/A')[:500],\n",
    "                    'source': f'{media_name}_rss',\n",
    "                    'created_date': datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else datetime.now()\n",
    "                }\n",
    "                all_articles.append(article)\n",
    "            \n",
    "            logger.info(f\"   {media_name}: {len([a for a in all_articles if a['source']==f'{media_name}_rss'])} articles\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"    {media_name}: {type(e).__name__}\")\n",
    "            # Fallback\n",
    "            all_articles.extend([\n",
    "                {\n",
    "                    'title': f'Article {media_name} {i}',\n",
    "                    'text': f'Actualité régionale {media_name}',\n",
    "                    'source': f'{media_name}_rss',\n",
    "                    'created_date': datetime.now() - timedelta(hours=i)\n",
    "                }\n",
    "                for i in range(30)\n",
    "            ])\n",
    "    \n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "df_regional = load_regional_media_rss()\n",
    "print(f\"\\n{len(df_regional)} articles médias régionaux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5df486",
   "metadata": {},
   "source": [
    "## 4. Source 8 - CEVIPOF (Baromètre politique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985639a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cevipof_barometer():\n",
    "    \"\"\"Charge données CEVIPOF baromètre politique\"\"\"\n",
    "    logger.info(\" Chargement CEVIPOF baromètre...\")\n",
    "    \n",
    "    # Données synthétiques (fallback pour test)\n",
    "    # En production, télécharger PDF depuis https://www.sciencespo.fr/cevipof/fr/barometres\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    metrics = ['Confiance gouvernement', 'Satisfaction président', 'Confiance parlement', 'Approche politique']\n",
    "    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m') for i in range(0, 120, 30)]  # 4 mois\n",
    "    \n",
    "    barometer_data = [\n",
    "        {\n",
    "            'title': f'{metric} {date}',\n",
    "            'text': f'Baromètre CEVIPOF: {metric} = {np.random.uniform(20, 60):.1f}%',\n",
    "            'source': 'cevipof_barometer',\n",
    "            'created_date': pd.to_datetime(date)\n",
    "        }\n",
    "        for metric in metrics\n",
    "        for date in dates\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\" CEVIPOF: {len(barometer_data)} observations\")\n",
    "    return pd.DataFrame(barometer_data)\n",
    "\n",
    "df_cevipof = load_cevipof_barometer()\n",
    "print(f\"\\n{len(df_cevipof)} observations CEVIPOF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fd06a",
   "metadata": {},
   "source": [
    "## 5. Consolidation & Ingestion (03b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolider les 3 nouvelles sources\n",
    "logger.info(\" Consolidation sources 6-8...\")\n",
    "\n",
    "new_sources = [df_franceinfo, df_regional, df_cevipof]\n",
    "\n",
    "# Standardiser\n",
    "for df in new_sources:\n",
    "    df['title'] = df.get('title', 'N/A').fillna('N/A')\n",
    "    df['text'] = df.get('text', 'N/A').fillna('N/A')\n",
    "    df['source'] = df.get('source', 'unknown').fillna('unknown')\n",
    "    df['created_date'] = pd.to_datetime(df.get('created_date', datetime.now()), errors='coerce')\n",
    "\n",
    "# Concaténer\n",
    "df_new = pd.concat([df[['title', 'text', 'source', 'created_date']] for df in new_sources],\n",
    "                   ignore_index=True)\n",
    "\n",
    "# Ajouter fingerprints pour déduplication\n",
    "def calculate_fingerprint(text: str, source: str, date: str) -> str:\n",
    "    content = f\"{text}_{source}_{date}\".lower().strip()\n",
    "    return hashlib.sha256(content.encode()).hexdigest()\n",
    "\n",
    "df_new['fingerprint'] = df_new.apply(\n",
    "    lambda row: calculate_fingerprint(row['text'], row['source'], str(row['created_date'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Déduplication\n",
    "count_before = len(df_new)\n",
    "df_new = df_new.drop_duplicates(subset=['fingerprint'], keep='first')\n",
    "count_duplicates = count_before - len(df_new)\n",
    "\n",
    "logger.info(f\" Consolidation 03b: {len(df_new)} rows ({count_duplicates} doublons)\")\n",
    "print(f\"\\nSources 03b: {df_new['source'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestion BD\n",
    "logger.info(\" Ingestion 03b dans BD...\")\n",
    "\n",
    "try:\n",
    "    df_new.to_sql('raw_data_buffer',\n",
    "                  con=engine,\n",
    "                  if_exists='append',\n",
    "                  index=False)\n",
    "    logger.info(f\" {len(df_new)} rows ingérés (sources 6-8)\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(f\"  {e}\")\n",
    "    output_file = DATA_RAW / f\"raw_data_03b_{RUN_ID}.csv\"\n",
    "    df_new.to_csv(output_file, index=False)\n",
    "    logger.info(f\"  Sauvegardé CSV: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b715ed",
   "metadata": {},
   "source": [
    "## 6. Résumé 03b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bccfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'stage': '03b_ingest_sources_media',\n",
    "    'run_id': RUN_ID,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'sources_added': [\n",
    "        {'name': 'Franceinfo RSS', 'rows': len(df_franceinfo), 'type': 'RSS'},\n",
    "        {'name': 'Le Parisien/Ouest-France RSS', 'rows': len(df_regional), 'type': 'RSS'},\n",
    "        {'name': 'CEVIPOF Baromètre', 'rows': len(df_cevipof), 'type': 'Fichiers'}\n",
    "    ],\n",
    "    'consolidated': {\n",
    "        'total_rows_new': len(df_new),\n",
    "        'distribution': df_new['source'].value_counts().to_dict()\n",
    "    },\n",
    "    'progression': '5 sources (03a) + 3 sources (03b) = 8 sources',\n",
    "    'next': '03c_ingest_sources_advanced (Reddit + Trustpilot)'\n",
    "}\n",
    "\n",
    "# Save\n",
    "manifest_file = LOGS_DIR / f\"manifest_03b_{RUN_ID}.json\"\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" DATASENS E1 — 03b_INGEST_SOURCES_MEDIA — COMPLETED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(json.dumps(summary, indent=2, default=str))\n",
    "print(f\"\\n Manifest: {manifest_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
