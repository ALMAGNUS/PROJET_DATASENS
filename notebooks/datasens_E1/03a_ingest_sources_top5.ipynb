{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Imports & Config\nimport os\nimport sys\nimport json\nimport sqlite3\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport hashlib\nfrom dotenv import load_dotenv\nfrom sqlalchemy import create_engine, text\n\n# Load environment\nload_dotenv()\n\n# Database engines\nRAW_DB = 'sqlite:///datasens.db'\nengine_raw = create_engine(RAW_DB)\n\n# Directories\nDATA_RAW = Path('data/raw')\nDATA_RAW.mkdir(parents=True, exist_ok=True)\n\nprint(\" Setup complete. Ready to ingest sources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n\ndef compute_fingerprint(title: str, content: str) -> str:\n    \"\"\"SHA256 fingerprint for deduplication\"\"\"\n    combined = f\"{title.strip()}{content.strip()}\".lower()\n    return hashlib.sha256(combined.encode()).hexdigest()\n\ndef store_raw_data(source_id: int, source_name: str, articles: list, date_str: str):\n    \"\"\"\n    Store articles in raw_data table with deduplication\n    \n    Args:\n        source_id: ID of the source\n        source_name: Name of the source (for folder structure)\n        articles: List of dicts with keys: title, content, url, published_at\n        date_str: YYYY-MM-DD format\n    \"\"\"\n    with engine_raw.connect() as conn:\n        for article in articles:\n            fingerprint = compute_fingerprint(article['title'], article['content'])\n            \n            # Check if already exists\n            existing = conn.execute(\n                text(\"SELECT raw_data_id FROM raw_data WHERE fingerprint = :fp\"),\n                {\"fp\": fingerprint}\n            ).fetchone()\n            \n            if not existing:\n                conn.execute(\n                    text(\"\"\"\n                        INSERT INTO raw_data \n                        (source_id, title, content, url, published_at, collected_at, fingerprint)\n                        VALUES (:src_id, :title, :content, :url, :pub_at, :col_at, :fp)\n                    \"\"\"),\n                    {\n                        \"src_id\": source_id,\n                        \"title\": article.get('title', '')[:500],\n                        \"content\": article.get('content', '')[:5000],\n                        \"url\": article.get('url', '')[:1000],\n                        \"pub_at\": article.get('published_at', datetime.now()),\n                        \"col_at\": datetime.now(),\n                        \"fp\": fingerprint\n                    }\n                )\n        conn.commit()\n\ndef log_sync(source_id: int, source_name: str, row_count: int, status: str = \"success\"):\n    \"\"\"Log sync operation\"\"\"\n    with engine_raw.connect() as conn:\n        conn.execute(\n            text(\"\"\"\n                INSERT INTO sync_log (source_id, sync_date, rows_synced, status, error_msg)\n                VALUES (:src_id, :sync_date, :rows, :status, NULL)\n            \"\"\"),\n            {\n                \"src_id\": source_id,\n                \"sync_date\": datetime.now(),\n                \"rows\": row_count,\n                \"status\": status\n            }\n        )\n        conn.commit()\n\nprint(\" Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e72603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SOURCE 1: RSS_FRENCH_NEWS (Feedparser)\nimport feedparser\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"  RSS_FRENCH_NEWS (Feedparser)\")\nprint(\"=\"*60)\n\n# Register source\nwith engine_raw.connect() as conn:\n    result = conn.execute(\n        text(\"SELECT source_id FROM source WHERE name = 'rss_french_news'\")\n    ).fetchone()\n    \n    if not result:\n        conn.execute(\n            text(\"\"\"\n                INSERT INTO source (name, source_type, url, description)\n                VALUES ('rss_french_news', 'RSS', 'https://www.france24.com/fr/en-direct/rss', \n                        'News from France24 RSS feed')\n            \"\"\")\n        )\n        conn.commit()\n        result = conn.execute(\n            text(\"SELECT source_id FROM source WHERE name = 'rss_french_news'\")\n        ).fetchone()\n    \n    source_id = result[0]\n\n# Fetch RSS\ntry:\n    feed_url = \"https://www.france24.com/fr/en-direct/rss\"\n    feed = feedparser.parse(feed_url)\n    \n    articles = []\n    for entry in feed.entries[:50]:  # Limit to 50 for speed\n        articles.append({\n            'title': entry.get('title', 'N/A'),\n            'content': entry.get('summary', '')[:1000],\n            'url': entry.get('link', ''),\n            'published_at': datetime(*entry.get('published_parsed', datetime.now().timetuple())[:6]) \n                           if 'published_parsed' in entry else datetime.now()\n        })\n    \n    store_raw_data(source_id, 'rss_french_news', articles, datetime.now().strftime(\"%Y-%m-%d\"))\n    log_sync(source_id, 'rss_french_news', len(articles), 'success')\n    \n    print(f\" Ingested {len(articles)} articles from rss_french_news\")\nexcept Exception as e:\n    print(f\" Error: {e}\")\n    log_sync(source_id, 'rss_french_news', 0, f'error: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796eed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SOURCE 2: GDELT_EVENTS (API)\nimport requests\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"  GDELT_EVENTS (API)\")\nprint(\"=\"*60)\n\n# Register source\nwith engine_raw.connect() as conn:\n    result = conn.execute(\n        text(\"SELECT source_id FROM source WHERE name = 'gdelt_events'\")\n    ).fetchone()\n    \n    if not result:\n        conn.execute(\n            text(\"\"\"\n                INSERT INTO source (name, source_type, url, description)\n                VALUES ('gdelt_events', 'API', 'https://api.gdeltproject.org/api/v2/', \n                        'Global Event Data Lab - Event database')\n            \"\"\")\n        )\n        conn.commit()\n        result = conn.execute(\n            text(\"SELECT source_id FROM source WHERE name = 'gdelt_events'\")\n        ).fetchone()\n    \n    source_id = result[0]\n\ntry:\n    # GDELT API query\n    url = \"https://api.gdeltproject.org/api/v2/search/tv\"\n    params = {\n        \"keyword\": \"France OR French\",\n        \"format\": \"json\",\n        \"orderby\": \"date\",\n        \"mode\": \"ArtList\",\n        \"maxrecords\": 100\n    }\n    \n    response = requests.get(url, params=params, timeout=10)\n    data = response.json()\n    \n    articles = []\n    if 'articles' in data:\n        for article in data['articles'][:50]:\n            articles.append({\n                'title': article.get('title', 'N/A'),\n                'content': article.get('description', '')[:1000],\n                'url': article.get('url', ''),\n                'published_at': pd.to_datetime(article.get('dateadded', datetime.now()))\n            })\n    \n    store_raw_data(source_id, 'gdelt_events', articles, datetime.now().strftime(\"%Y-%m-%d\"))\n    log_sync(source_id, 'gdelt_events', len(articles), 'success')\n    \n    print(f\" Ingested {len(articles)} events from GDELT\")\nexcept Exception as e:\n    print(f\" Error: {e}\")\n    log_sync(source_id, 'gdelt_events', 0, f'error: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d58f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SOURCE 3: OPENWEATHER_API (Requires key)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"  OPENWEATHER_API\")\nprint(\"=\"*60)\n\n# Register source\nwith engine_raw.connect() as conn:\n    result = conn.execute(\n        text(\"SELECT source_id FROM source WHERE name = 'openweather_api'\")\n    ).fetchone()\n    \n    if not result:\n        conn.execute(\n            text(\"\"\"\n                INSERT INTO source (name, source_type, url, description)\n                VALUES ('openweather_api', 'API', 'https://openweathermap.org/api', \n                        'OpenWeather API - Weather data for French cities')\n            \"\"\")\n        )\n        conn.commit()\n        result = conn.execute(\n            text(\"SELECT source_id FROM source WHERE name = 'openweather_api'\")\n        ).fetchone()\n    \n    source_id = result[0]\n\ntry:\n    api_key = os.getenv('OPENWEATHER_API_KEY', '')\n    \n    if not api_key:\n        print(\"  OPENWEATHER_API_KEY not in .env. Using MOCK data.\")\n        # MOCK: Simulate weather data for French cities\n        cities = ['Paris', 'Lyon', 'Marseille', 'Toulouse', 'Nice']\n        articles = []\n        for city in cities:\n            for day in range(5):\n                date = datetime.now() - timedelta(days=day)\n                articles.append({\n                    'title': f\"Weather {city} — {date.strftime('%Y-%m-%d')}\",\n                    'content': f\"Temperature: {np.random.randint(0, 30)}°C, Condition: {'Sunny' if np.random.random() > 0.5 else 'Rainy'}\",\n                    'url': f\"https://openweathermap.org/{city}\",\n                    'published_at': date\n                })\n    else:\n        # Real API call\n        cities = [('Paris', 48.8566, 2.3522), ('Lyon', 45.7640, 4.8357)]\n        articles = []\n        \n        for city, lat, lon in cities:\n            url = \"https://api.openweathermap.org/data/2.5/weather\"\n            params = {\"lat\": lat, \"lon\": lon, \"appid\": api_key, \"units\": \"metric\"}\n            response = requests.get(url, params=params, timeout=10)\n            \n            if response.status_code == 200:\n                data = response.json()\n                articles.append({\n                    'title': f\"Weather {city} — {datetime.now().strftime('%Y-%m-%d')}\",\n                    'content': f\"Temp: {data['main']['temp']}°C, {data['weather'][0]['description']}\",\n                    'url': f\"https://openweathermap.org/{city}\",\n                    'published_at': datetime.now()\n                })\n    \n    store_raw_data(source_id, 'openweather_api', articles, datetime.now().strftime(\"%Y-%m-%d\"))\n    log_sync(source_id, 'openweather_api', len(articles), 'success')\n    \n    print(f\" Ingested {len(articles)} weather records\")\nexcept Exception as e:\n    print(f\" Error: {e}\")\n    log_sync(source_id, 'openweather_api', 0, f'error: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca516f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SOURCE 4: INSEE_API (Public)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"  INSEE_API (Public data)\")\nprint(\"=\"*60)\n\n# Register source\nwith engine_raw.connect() as conn:\n    result = conn.execute(\n        text(\"SELECT source_id FROM source WHERE name = 'insee_api'\")\n    ).fetchone()\n    \n    if not result:\n        conn.execute(\n            text(\"\"\"\n                INSERT INTO source (name, source_type, url, description)\n                VALUES ('insee_api', 'API', 'https://www.insee.fr/api/', \n                        'INSEE - French statistical agency data')\n            \"\"\")\n        )\n        conn.commit()\n        result = conn.execute(\n            text(\"SELECT source_id FROM source WHERE name = 'insee_api'\")\n        ).fetchone()\n    \n    source_id = result[0]\n\ntry:\n    # INSEE public data endpoint\n    url = \"https://api.insee.fr/catalogue/datasets\"\n    \n    # Using MOCK for simplicity (INSEE requires auth for full data)\n    print(\"  INSEE API requires authentication. Using MOCK statistical data.\")\n    \n    articles = []\n    for month in range(1, 13):\n        articles.append({\n            'title': f\"INSEE Stats — 2025-{month:02d}\",\n            'content': f\"Monthly economic indicators: Employment {90 + np.random.randint(-2, 3)}%, Inflation {2.1 + np.random.random()}%\",\n            'url': \"https://www.insee.fr/\",\n            'published_at': datetime(2025, month, 1)\n        })\n    \n    store_raw_data(source_id, 'insee_api', articles, datetime.now().strftime(\"%Y-%m-%d\"))\n    log_sync(source_id, 'insee_api', len(articles), 'success')\n    \n    print(f\" Ingested {len(articles)} INSEE statistics\")\nexcept Exception as e:\n    print(f\" Error: {e}\")\n    log_sync(source_id, 'insee_api', 0, f'error: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SOURCE 5: KAGGLE_FRENCH_OPINIONS (Manual CSV OR MOCK)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"  KAGGLE_FRENCH_OPINIONS (CSV OR MOCK)\")\nprint(\"=\"*60)\n\n# Register source\nwith engine_raw.connect() as conn:\n    result = conn.execute(\n        text(\"SELECT source_id FROM source WHERE name = 'kaggle_french_opinions'\")\n    ).fetchone()\n    \n    if not result:\n        conn.execute(\n            text(\"\"\"\n                INSERT INTO source (name, source_type, url, description)\n                VALUES ('kaggle_french_opinions', 'Dataset', 'https://www.kaggle.com/', \n                        'Kaggle - French product reviews & opinions')\n            \"\"\")\n        )\n        conn.commit()\n        result = conn.execute(\n            text(\"SELECT source_id FROM source WHERE name = 'kaggle_french_opinions'\")\n        ).fetchone()\n    \n    source_id = result[0]\n\ntry:\n    kaggle_path = DATA_RAW / 'kaggle_french_opinions'\n    csv_files = list(kaggle_path.glob('**/opinions*.csv'))\n    \n    if csv_files:\n        print(f\" Found Kaggle CSV: {csv_files[0]}\")\n        df = pd.read_csv(csv_files[0], nrows=1000)  # Limit to 1000 rows\n        \n        articles = []\n        for _, row in df.iterrows():\n            articles.append({\n                'title': str(row.get('title', row.get('review_title', 'Opinion')))[:500],\n                'content': str(row.get('review', row.get('content', '')))[:1000],\n                'url': str(row.get('url', ''))[:1000],\n                'published_at': pd.to_datetime(row.get('date', datetime.now()), errors='coerce') or datetime.now()\n            })\n    else:\n        print(\"  Kaggle CSV not found. Generating MOCK French opinions...\")\n        \n        products = ['iPhone 14', 'Samsung TV', 'Nike Air Max', 'Dyson Vacuum', 'Nespresso Machine']\n        articles = []\n        \n        for i in range(500):\n            product = np.random.choice(products)\n            rating = np.random.randint(1, 6)\n            articles.append({\n                'title': f\"Opinion: {product} - {rating}/5 stars\",\n                'content': f\"Avis utilisateur #{i}: Ce produit est {'excellent' if rating >= 4 else 'moyen'}. \" + \n                          f\"Qualité: {'très bonne' if rating >= 4 else 'acceptable'}, Service: {'rapide' if np.random.random() > 0.5 else 'lent'}\",\n                'url': f\"https://kaggle.com/opinions/{i}\",\n                'published_at': datetime.now() - timedelta(days=np.random.randint(0, 365))\n            })\n    \n    store_raw_data(source_id, 'kaggle_french_opinions', articles, datetime.now().strftime(\"%Y-%m-%d\"))\n    log_sync(source_id, 'kaggle_french_opinions', len(articles), 'success')\n    \n    print(f\" Ingested {len(articles)} opinions from Kaggle\")\nexcept Exception as e:\n    print(f\" Error: {e}\")\n    log_sync(source_id, 'kaggle_french_opinions', 0, f'error: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a87995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY\n\nprint(\"\\n\" + \"=\"*60)\nprint(\" INGESTION SUMMARY (03a)\")\nprint(\"=\"*60)\n\nwith engine_raw.connect() as conn:\n    sources = conn.execute(text(\"SELECT COUNT(*) FROM source\")).fetchone()[0]\n    articles = conn.execute(text(\"SELECT COUNT(*) FROM raw_data\")).fetchone()[0]\n    syncs = conn.execute(text(\"SELECT COUNT(*) FROM sync_log\")).fetchone()[0]\n\nprint(f\" Sources registered: {sources}\")\nprint(f\" Total articles ingested: {articles}\")\nprint(f\" Sync operations logged: {syncs}\")\n\nprint(\"\\n Next step: Run 03b_ingest_sources_media.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}