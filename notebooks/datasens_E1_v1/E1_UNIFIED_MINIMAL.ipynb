{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bf3219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, sqlite3, hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, text\n",
    "from pathlib import Path\n",
    "\n",
    "# Single DB for E1\n",
    "RAW = create_engine('sqlite:///datasens.db')\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91ecb6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema created + 10 sources inserted\n"
     ]
    }
   ],
   "source": [
    "# 02: SCHEMA (6 CORE TABLES - E1 ONLY)\n",
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS source (\n",
    "    source_id INTEGER PRIMARY KEY,\n",
    "    name TEXT UNIQUE,\n",
    "    source_type TEXT,\n",
    "    url TEXT,\n",
    "    sync_frequency TEXT,\n",
    "    last_sync_date TIMESTAMP,\n",
    "    retry_policy TEXT,\n",
    "    active BOOLEAN\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS raw_data (raw_data_id INTEGER PRIMARY KEY, source_id INTEGER, title TEXT, content TEXT, url TEXT, published_at TIMESTAMP, collected_at TIMESTAMP, fingerprint TEXT UNIQUE, quality_score REAL DEFAULT 0.5);\n",
    "CREATE TABLE IF NOT EXISTS sync_log (sync_log_id INTEGER PRIMARY KEY, source_id INTEGER, sync_date TIMESTAMP, rows_synced INTEGER, status TEXT);\n",
    "CREATE TABLE IF NOT EXISTS topic (topic_id INTEGER PRIMARY KEY, name TEXT UNIQUE, keywords TEXT);\n",
    "CREATE TABLE IF NOT EXISTS document_topic (doc_topic_id INTEGER PRIMARY KEY, raw_data_id INTEGER, topic_id INTEGER);\n",
    "CREATE TABLE IF NOT EXISTS model_output (output_id INTEGER PRIMARY KEY, raw_data_id INTEGER, model_name TEXT, label TEXT, score REAL);\n",
    "\"\"\"\n",
    "\n",
    "for stmt in sql.split(';'):\n",
    "    if stmt.strip():\n",
    "        with RAW.connect() as conn:\n",
    "            conn.execute(text(stmt))\n",
    "            conn.commit()\n",
    "\n",
    "# INSERT 10 SOURCES\n",
    "sources = [\n",
    "    ('rss_french_news', 'RSS', 'https://www.lemonde.fr/rss/une.xml', 'DAILY', 'SKIP'),\n",
    "    ('gdelt_events', 'API', 'https://api.gdeltproject.org/api/v2/search/tv', 'DAILY', 'SKIP'),\n",
    "    ('openweather_api', 'API', 'https://openweathermap.org/api', 'DAILY', 'SKIP'),\n",
    "    ('insee_api', 'API', 'https://www.insee.fr/fr/accueil', 'MONTHLY', 'SKIP'),\n",
    "    ('kaggle_french_opinions', 'Dataset', 'https://kaggle.com/datasets/french-opinions', 'MONTHLY', 'SKIP'),\n",
    "    ('google_news_rss', 'RSS', 'https://news.google.com/rss', 'DAILY', 'SKIP'),\n",
    "    ('regional_media_rss', 'RSS', 'https://www.bfmtv.com/rss/', 'DAILY', 'SKIP'),\n",
    "    ('ifop_barometers', 'WebScraping', 'https://www.ifop.com/', 'YEARLY', 'FALLBACK_PREVIOUS_YEAR'),\n",
    "    ('reddit_france', 'API', 'https://api.reddit.com/r/france', 'DAILY', 'SKIP'),\n",
    "    ('trustpilot_reviews', 'WebScraping', 'https://www.trustpilot.com/', 'WEEKLY', 'SKIP'),\n",
    "]\n",
    "\n",
    "with RAW.connect() as conn:\n",
    "    for name, stype, url, freq, retry in sources:\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT OR IGNORE INTO source (name, source_type, url, sync_frequency, retry_policy, active)\n",
    "            VALUES (:name, :type, :url, :freq, :retry, 1)\n",
    "        \"\"\"), {'name': name, 'type': stype, 'url': url, 'freq': freq, 'retry': retry})\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Schema created + 10 sources inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a866bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 active sources\n",
      "INGEST rss_french_news... 2 articles\n",
      "INGEST gdelt_events... 2 articles\n",
      "INGEST openweather_api... 2 articles\n",
      "INGEST insee_api... 2 articles\n",
      "INGEST kaggle_french_opinions... 2 articles\n",
      "INGEST google_news_rss... 2 articles\n",
      "INGEST regional_media_rss... 2 articles\n",
      "INGEST ifop_barometers... 2 articles\n",
      "INGEST reddit_france... 2 articles\n",
      "INGEST trustpilot_reviews... 2 articles\n",
      "\n",
      "TOTAL INGESTED: 20 articles\n"
     ]
    }
   ],
   "source": [
    "# 03: INGEST (Smart scheduling with mock realistic data)\n",
    "import feedparser, requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# Mock data for testing\n",
    "MOCK_ARTICLES = [\n",
    "    {'title': 'Sentiment francais sur economie', 'content': 'Les Francais sont preoccupes par linflation', 'url': 'https://example.com/1', 'published_at': '2025-12-10'},\n",
    "    {'title': 'Sondage barometrique climat', 'content': 'Etude montrant la prise de conscience sur le changement climatique', 'url': 'https://example.com/2', 'published_at': '2025-12-09'},\n",
    "    {'title': 'Politique europeenne tensions', 'content': 'Nouvelles tensions diplomatiques', 'url': 'https://example.com/3', 'published_at': '2025-12-08'},\n",
    "    {'title': 'Sante publique enjeux', 'content': 'Debats autour de la reforme du systeme de sante', 'url': 'https://example.com/4', 'published_at': '2025-12-07'},\n",
    "    {'title': 'Technologie IA France', 'content': 'La France accelere ses investissements dans IA', 'url': 'https://example.com/5', 'published_at': '2025-12-06'},\n",
    "    {'title': 'Emploi jeunesse statistiques', 'content': 'Taux de chomage des jeunes en baisse', 'url': 'https://example.com/6', 'published_at': '2025-12-05'},\n",
    "    {'title': 'Immigration debat societal', 'content': 'Nouvelle enquete dopinion', 'url': 'https://example.com/7', 'published_at': '2025-12-04'},\n",
    "    {'title': 'Ecole education reforme', 'content': 'Resultats scolaires post-reforme', 'url': 'https://example.com/8', 'published_at': '2025-12-03'},\n",
    "    {'title': 'Transport mobilite durable', 'content': 'Transition vers transports propres', 'url': 'https://example.com/9', 'published_at': '2025-12-02'},\n",
    "    {'title': 'Culture patrimoine francais', 'content': 'Preservation du patrimoine cultural', 'url': 'https://example.com/10', 'published_at': '2025-12-01'},\n",
    "    {'title': 'Sport ligue 1 sentiment', 'content': 'Passion des supporters francais', 'url': 'https://example.com/11', 'published_at': '2025-11-30'},\n",
    "    {'title': 'Tourisme France attractivite', 'content': 'Tendances du tourisme', 'url': 'https://example.com/12', 'published_at': '2025-11-29'},\n",
    "]\n",
    "\n",
    "# Get active sources from DB\n",
    "with RAW.connect() as conn:\n",
    "    active_sources = conn.execute(text(\"SELECT source_id, name FROM source WHERE active = 1\")).fetchall()\n",
    "\n",
    "print(f\"Found {len(active_sources)} active sources\")\n",
    "\n",
    "# INGEST\n",
    "total_ingested = 0\n",
    "with RAW.connect() as conn:\n",
    "    for idx, (source_id, source_name) in enumerate(active_sources):\n",
    "        # Get mock articles (rotate through mock data)\n",
    "        articles = MOCK_ARTICLES[idx*2:(idx*2)+2] if idx*2 < len(MOCK_ARTICLES) else MOCK_ARTICLES[:2]\n",
    "        \n",
    "        print(f\"INGEST {source_name}...\", end=\" \")\n",
    "        \n",
    "        inserted = 0\n",
    "        for article in articles:\n",
    "            try:\n",
    "                fp = hashlib.sha256((article['title'] + article['url']).encode()).hexdigest()\n",
    "                conn.execute(text(\"\"\"\n",
    "                    INSERT OR IGNORE INTO raw_data \n",
    "                    (source_id, title, content, url, fingerprint, published_at, collected_at, quality_score)\n",
    "                    VALUES (:source_id, :title, :content, :url, :fp, :published, datetime('now'), 0.8)\n",
    "                \"\"\"), {\n",
    "                    'source_id': source_id,\n",
    "                    'title': article['title'],\n",
    "                    'content': article['content'],\n",
    "                    'url': article['url'],\n",
    "                    'fp': fp,\n",
    "                    'published': article['published_at']\n",
    "                })\n",
    "                inserted += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"{inserted} articles\")\n",
    "        total_ingested += inserted\n",
    "\n",
    "print(f\"\\nTOTAL INGESTED: {total_ingested} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1939c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check: 12/12 articles passed\n"
     ]
    }
   ],
   "source": [
    "# 04: QUALITY CHECK (in-memory)\n",
    "df = pd.read_sql(\"SELECT * FROM raw_data\", RAW)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Normalize\n",
    "    df['title'] = df['title'].fillna('').str.strip()\n",
    "    df['content'] = df['content'].fillna('').str.strip()\n",
    "    \n",
    "    # Quality scoring\n",
    "    def quality(row):\n",
    "        score = 0.5\n",
    "        if len(str(row['title']).strip()) > 10: score += 0.2\n",
    "        if len(str(row['content']).strip()) > 50: score += 0.3\n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    df['quality_score'] = df.apply(quality, axis=1)\n",
    "    \n",
    "    # Dedup detection (simple set-based)\n",
    "    def fp(text):\n",
    "        return hashlib.sha256(str(text).encode()).hexdigest()\n",
    "    \n",
    "    seen = set()\n",
    "    duplicates = []\n",
    "    for idx, row in df.iterrows():\n",
    "        hash_val = fp(row['title'] + row['content'])\n",
    "        if hash_val in seen:\n",
    "            duplicates.append(True)\n",
    "        else:\n",
    "            seen.add(hash_val)\n",
    "            duplicates.append(False)\n",
    "    \n",
    "    df['is_duplicate'] = duplicates\n",
    "    \n",
    "    # Filter low quality\n",
    "    df_clean = df[~df['is_duplicate'] & (df['quality_score'] >= 0.5)].copy()\n",
    "    \n",
    "    print(f\"Quality check: {len(df_clean)}/{len(df)} articles passed\")\n",
    "else:\n",
    "    df_clean = df\n",
    "    print(\"No raw data to clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfed10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "E1 PIPELINE COMPLETE\n",
      "============================================================\n",
      "RAW: 10 sources, 12 articles\n",
      "Quality: 12 articles passed filters\n",
      "Compression: 0.0% removed\n",
      "Topics: 0 defined\n",
      "\n",
      "Ready for EXPORT GOLD (Parquet)\n"
     ]
    }
   ],
   "source": [
    "# 05: METRICS & SUMMARY\n",
    "with RAW.connect() as conn:\n",
    "    raw_count = conn.execute(text(\"SELECT COUNT(*) FROM raw_data\")).scalar()\n",
    "    raw_sources = conn.execute(text(\"SELECT COUNT(*) FROM source\")).scalar()\n",
    "    topics_count = conn.execute(text(\"SELECT COUNT(*) FROM topic\")).scalar()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"E1 PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RAW: {raw_sources} sources, {raw_count:,} articles\")\n",
    "print(f\"Quality: {len(df_clean):,} articles passed filters\")\n",
    "if len(df) > 0:\n",
    "    print(f\"Compression: {100*(1-len(df_clean)/len(df)):.1f}% removed\")\n",
    "else:\n",
    "    print(\"Compression: No data to compress\")\n",
    "print(f\"Topics: {topics_count} defined\")\n",
    "print(f\"\\nReady for EXPORT GOLD (Parquet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38599438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GOLD ZONE EXPORTED (Ready for IA)\n",
      "============================================================\n",
      "Total rows: 12\n",
      "Partitions: 12\n",
      "Format: Parquet (snappy)\n",
      "Location: data\\gold\\articles_parquet\n",
      "Columns: 13\n",
      "\n",
      "Dataset ready for fine-tuning IA models!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 05: EXPORT GOLD (Parquet) - DATASET PRET POUR IA\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Load all data\n",
    "df_raw = pd.read_sql(\"SELECT * FROM raw_data\", RAW)\n",
    "df_sources = pd.read_sql(\"SELECT * FROM source\", RAW)\n",
    "df_topics = pd.read_sql(\"SELECT * FROM topic\", RAW)\n",
    "df_doc_topics = pd.read_sql(\"SELECT * FROM document_topic\", RAW)\n",
    "df_models = pd.read_sql(\"SELECT * FROM model_output\", RAW)\n",
    "\n",
    "# Join sources\n",
    "df_gold = df_raw.merge(df_sources[['source_id', 'name', 'source_type']], on='source_id', how='left')\n",
    "df_gold.rename(columns={'name': 'source_name'}, inplace=True)\n",
    "\n",
    "# Join topics (multiple topics per article)\n",
    "df_doc_topics_named = df_doc_topics.merge(df_topics[['topic_id', 'name']], on='topic_id', how='left')\n",
    "df_topics_agg = df_doc_topics_named.groupby('raw_data_id')['name'].apply(lambda x: '|'.join(x)).reset_index()\n",
    "df_topics_agg.rename(columns={'name': 'topics'}, inplace=True)\n",
    "df_gold = df_gold.merge(df_topics_agg, on='raw_data_id', how='left')\n",
    "df_gold['topics'] = df_gold['topics'].fillna('')\n",
    "\n",
    "# Join model outputs (sentiment/labels)\n",
    "df_models_latest = df_models.sort_values('output_id').drop_duplicates('raw_data_id', keep='last')\n",
    "df_gold = df_gold.merge(df_models_latest[['raw_data_id', 'model_name', 'label', 'score']], on='raw_data_id', how='left')\n",
    "df_gold.rename(columns={'label': 'sentiment_label', 'score': 'sentiment_score'}, inplace=True)\n",
    "\n",
    "# Add partition column\n",
    "df_gold['partition_date'] = pd.to_datetime(df_gold['published_at']).dt.date\n",
    "\n",
    "# Clean columns for export\n",
    "df_gold_export = df_gold[[\n",
    "    'raw_data_id', 'source_id', 'source_name', 'source_type',\n",
    "    'title', 'content', 'url', 'published_at', 'collected_at',\n",
    "    'topics', 'sentiment_label', 'sentiment_score',\n",
    "    'partition_date'\n",
    "]].copy()\n",
    "\n",
    "# Create GOLD directory\n",
    "gold_dir = Path('data/gold/articles_parquet')\n",
    "gold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export to Parquet (partitioned by date)\n",
    "df_gold_export.to_parquet(\n",
    "    gold_dir,\n",
    "    partition_cols=['partition_date'],\n",
    "    compression='snappy',\n",
    "    index=False,\n",
    "    engine='pyarrow'\n",
    ")\n",
    "\n",
    "# Verify\n",
    "partitions = list(gold_dir.glob('partition_date=*/'))\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"GOLD ZONE EXPORTED (Ready for IA)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows: {len(df_gold_export):,}\")\n",
    "print(f\"Partitions: {len(partitions)}\")\n",
    "print(f\"Format: Parquet (snappy)\")\n",
    "print(f\"Location: {gold_dir}\")\n",
    "print(f\"Columns: {len(df_gold_export.columns)}\")\n",
    "print(f\"\\nDataset ready for fine-tuning IA models!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
