# Guide de Contribution ‚Äî DataSens

Merci d'envisager de contribuer √† **DataSens** ! Ce document guide comment participer au projet.

---

## Code de Conduite

Nous nous engageons √† fournir un environnement accueillant et inclusif. Tous les contributeurs doivent respecter le [Code de Conduite](CODE_OF_CONDUCT.md).

---

## Comment Contribuer

### üìã Signaler un Bug

1. **Avant de rapporter**, v√©rifiez que le bug n'existe pas d√©j√† dans [Issues](https://github.com/ALMAGNUS/PROJET_DATASENS/issues)
2. **Incluez**: Description, √©tapes de reproduction, r√©sultat attendu, r√©sultat obtenu
3. **Annexez**: Logs (sync_log, cleaning_audit), versions Python/OS
4. **Exemple**:
   ```
   **Bug**: SILVER zone contient 0 articles
   **√âtapes**:
   1. Lanc√© python quick_start.py
   2. V√©rification datasens_cleaned.db
   **R√©sultat attendu**: ~3500 articles nettoy√©s
   **R√©sultat obtenu**: 0 articles
   **Logs**: [Voir cleaning_audit table]
   **Env**: Python 3.9, Windows 11
   ```

### üí° Proposer une Fonctionnalit√©

1. **Ouvrir Discussion** sur [Discussions](https://github.com/ALMAGNUS/PROJET_DATASENS/discussions)
2. **D√©crire**: Cas d'usage, b√©n√©fice, impl√©mentation propos√©e
3. **Exemple**:
   ```
   **Titre**: Support Streaming Kafka pour Phase 05
   **Cas d'usage**: Ingestion temps r√©el ~1M articles/jour
   **Approche**: Ajouter kafka_consumer() dans quick_start.py
   **Impact**: +50 lignes code max (keep minimaliste)
   ```

### üîß Soumettre une Pull Request

#### 1. Fork & Clone
```powershell
git clone https://github.com/[TON_COMPTE]/PROJET_DATASENS.git
cd PROJET_DATASENS
git checkout -b feature/ma-feature
```

#### 2. Faire les Changements

**Standards de code**:
- ‚úÖ **Minimalisme**: Garder < 500 lignes total (E1)
- ‚úÖ **PEP 8**: Format Python standard
- ‚úÖ **Documentation**: Commenter code complexe
- ‚úÖ **Tests**: CRUD tests pour nouvelles features
- ‚úÖ **Logs**: Tra√ßabilit√© compl√®te (sync_log + audit)

**Exemples de contributions accept√©es**:
- ‚úÖ Nouvelles sources (RSS, API) ‚Äî +10 lignes
- ‚úÖ Am√©lioration qualit√© scoring ‚Äî Optimisation existante
- ‚úÖ Nouveaux dashboards ‚Äî Dans visualize_dashboard.py
- ‚úÖ Documentation (README, LOGGING) ‚Äî Pas de limite code

**Exemples refus√©s** (hors scope):
- ‚ùå Tensorflow/Keras (Phase 06 seulement)
- ‚ùå Spark integrations (E2 seulement)
- ‚ùå Microservices (E3 seulement)

#### 3. Tester Localement
```powershell
python quick_start.py
# V√©rifier: datasens.db + datasens_cleaned.db existent
# V√©rifier: rapport_complet_e1.html g√©n√©r√©
# V√©rifier: sync_log et cleaning_audit remplis
```

#### 4. Commit avec Message Clair
```powershell
git add .
git commit -m "feat: ajoute source reddit_france + 10 tests"
git push origin feature/ma-feature
```

**Format commit** (Conventional Commits):
- `feat:` Nouvelle fonctionnalit√©
- `fix:` Correction de bug
- `docs:` Documentation (README, CHANGELOG)
- `refactor:` Am√©lioration code (pas de feature)
- `test:` Ajout/am√©lioration tests
- `perf:` Performance optimization
- `chore:` D√©pendances, setup

#### 5. Ouvrir Pull Request

Sur [Pull Requests](https://github.com/ALMAGNUS/PROJET_DATASENS/pulls):

```markdown
## Description
Br√®ve description de la PR

## Type de Changement
- [ ] Bug fix
- [ ] Nouvelle source
- [ ] Dashboard improvement
- [ ] Documentation
- [ ] Autre: ____

## V√©rification
- [ ] Code suit PEP 8
- [ ] Tests CRUD passent
- [ ] CHANGELOG.md mis √† jour
- [ ] README.md mis √† jour si applicable
- [ ] Logs (sync_log) document√©s si applicable

## Exemple d'Ex√©cution
```
python quick_start.py
RAW: 5240 articles
SILVER: 4150 articles (quality ‚â• 0.5)
```
```

---

## Structure du Projet

```
PROJET_DATASENS/
‚îú‚îÄ‚îÄ E1_UNIFIED_MINIMAL.ipynb      # Pipeline principal
‚îú‚îÄ‚îÄ quick_start.py                 # Orchestrateur
‚îú‚îÄ‚îÄ visualize_dashboard.py          # Dashboards
‚îú‚îÄ‚îÄ datasens.db                     # SQLite RAW
‚îú‚îÄ‚îÄ datasens_cleaned.db             # SQLite SILVER
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                        # Articles bruts (source/date/)
‚îÇ   ‚îî‚îÄ‚îÄ gold/                       # Parquet (Phase 05)
‚îú‚îÄ‚îÄ visualizations/                 # PNG + HTML outputs
‚îú‚îÄ‚îÄ README.md                       # Documentation principale
‚îú‚îÄ‚îÄ CHANGELOG.md                    # Historique versions
‚îú‚îÄ‚îÄ CONTRIBUTING.md                 # Ce fichier
‚îú‚îÄ‚îÄ LICENSE.md                      # MIT License
‚îî‚îÄ‚îÄ LOGGING.md                      # Documentation logs
```

---

## Documentation Logs

### SYNC_LOG Table

Documente chaque ingestion de donn√©es:

```sql
CREATE TABLE sync_log (
    sync_id INTEGER PRIMARY KEY,
    source_name TEXT NOT NULL,
    sync_start TIMESTAMP,
    sync_end TIMESTAMP,
    articles_fetched INTEGER,
    articles_inserted INTEGER,
    errors_count INTEGER,
    status TEXT,  -- 'SUCCESS', 'PARTIAL', 'FAILED'
    error_message TEXT,
    metadata JSON
);
```

**Exemple d'Entr√©e**:
```json
{
    "sync_id": 1,
    "source_name": "rss_french_news",
    "sync_start": "2025-12-15 10:00:00",
    "sync_end": "2025-12-15 10:05:00",
    "articles_fetched": 500,
    "articles_inserted": 485,
    "errors_count": 15,
    "status": "PARTIAL",
    "error_message": "15 articles with invalid URLs",
    "metadata": {"retry_count": 2, "timeout": false}
}
```

### CLEANING_AUDIT Table

Trace toutes les transformations:

```sql
CREATE TABLE cleaning_audit (
    audit_id INTEGER PRIMARY KEY,
    raw_article_id INTEGER,
    step_number INTEGER,
    step_name TEXT,
    action TEXT,
    value_before TEXT,
    value_after TEXT,
    timestamp TIMESTAMP
);
```

**√âtapes Track√©es**:
1. LOAD: Charge depuis RAW
2. NORMALIZE: Trim titre/contenu
3. PARSE_DATES: Parse published_at
4. QUALITY_SCORE: Calcule score
5. FINGERPRINT: D√©tecte doublons
6. FILTER: Applique threshold ‚â• 0.5
7. DEDUP: Marque duplicatas
8. INSERT: Ins√®re en SILVER
9. AUDIT_LOG: Log transformation
10. PARTITION: Cr√©e partition_date

### DATA_QUALITY_METRICS Table

R√©sume qualit√© par source:

```sql
CREATE TABLE data_quality_metrics (
    metric_id INTEGER PRIMARY KEY,
    source_name TEXT NOT NULL,
    metric_date DATE,
    total_articles INTEGER,
    avg_quality_score REAL,
    null_count INTEGER,
    duplicate_count INTEGER,
    invalid_urls INTEGER,
    incomplete_records INTEGER,
    processing_time_seconds REAL
);
```

---

## Directives de Code

### Python Style

```python
# ‚úÖ BON: Clair, minimaliste
def ingest(source_name):
    """Fetch articles from source"""
    try:
        data = fetch_source(source_name)
        return data or get_fallback_data(source_name)
    except Exception as e:
        log_error(source_name, str(e))
        return []

# ‚ùå MAUVAIS: Verbeux, sur-eng√©niris√©
def ingest(source_name, max_retries=3, timeout=10, 
           enable_cache=True, cache_duration=3600):
    # ... 50 lignes
```

### Tests

```python
# ‚úÖ CRUD Tests requis
assert len(raw_data) > 0  # CREATE
assert read_count(raw_db) > 0  # READ
assert quality_score_updated > 0  # UPDATE
assert duplicates_removed > 0  # DELETE
```

### Logs

```python
# ‚úÖ Log tout ce qui importe
sync_log.insert({
    'source': 'rss_french_news',
    'articles_fetched': 500,
    'articles_inserted': 485,
    'errors': 15,
    'status': 'PARTIAL'
})

# ‚ùå Pas assez de logs
print("Sync done")
```

---

## Processus de Review

1. **Automated Checks**:
   - ‚úÖ Code format (PEP 8)
   - ‚úÖ Tests CRUD passent
   - ‚úÖ Line count < 500 (E1)

2. **Code Review** (par maintainers):
   - Est-ce que √ßa suit la vision minimaliste ?
   - Documentation compl√®te ?
   - Logs appropri√©s ?
   - Impacts sur autres composants ?

3. **Merge**:
   - Approuv√©: Merge et release versioning
   - Refus√©: Feedback + suggestions d'am√©lioration

---

## Questions ?

- üêõ **Bug?** ‚Üí [Issues](https://github.com/ALMAGNUS/PROJET_DATASENS/issues)
- üí¨ **Discussion?** ‚Üí [Discussions](https://github.com/ALMAGNUS/PROJET_DATASENS/discussions)
- üìß **Contact?** ‚Üí [√Ä ajouter]

---

**Merci de contribuer √† DataSens!** üôå

**Derni√®re mise √† jour**: 15 d√©cembre 2025  
**Mainteneurs**: DataSens Team
